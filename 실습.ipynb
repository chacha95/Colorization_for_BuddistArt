{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gimikk/Colorization_for_BuddistArt/blob/master/%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SDL1okfxU4JM",
        "colab_type": "code",
        "outputId": "803cc1b7-3ad4-45bd-c0e8-80a10a8c7fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "cell_type": "code",
      "source": [
        "# pytorch 설치 명령어 입니다\n",
        "!pip3 install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 23kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x60b68000 @  0x7f424a06e2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 4.2MB/s \n",
            "\u001b[?25hInstalling collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-1.0.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3p2oLH-XU9ft",
        "colab_type": "code",
        "outputId": "dda3c4a5-e083-4df0-e18b-4e46e8faa6be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3969
        }
      },
      "cell_type": "code",
      "source": [
        "# Linear Regression\n",
        "# numpy는 numerical python의 약자로 행렬 계산에 특화된 package\n",
        "# matplotlib는 graph를 그려주는 등의 시각화 package\n",
        "# pandas는 데이터 분석, 데이터 처리를 위해 만들어진 package\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import io\n",
        "# 구글 드라이브에 csv 파일(엑셀)을 저장, 드라이브로 부터 읽어오는 함수\n",
        "# 구글 드라이브에 연결하기 위해 authorize 해야함!\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def loadData(fileName):\n",
        "  # np.loadtxt 함수로 csv 파일을 읽는다, delimiter는 파싱할 기준(,)\n",
        "  data = np.matrix(np.loadtxt(fname=fileName, delimiter=','))\n",
        "  # print를 찍어 데이터가 어떤지 확인해 보자\n",
        "  print(data)\n",
        "  # .shape를 이용해 shape를 알 수 있다.(모양)\n",
        "  print(data.shape)\n",
        "  # data행렬을 이해하고 그려보자!\n",
        "  return (data[:, 0], data[:, 1])\n",
        "\n",
        "def GradientDescent(X, Y, theta, alpha, num_iters):\n",
        "    m = np.size(Y)\n",
        "    hx = np.matmul(X, theta)\n",
        "    c1 = Cost(theta, X, Y)\n",
        "    for i in range(0, num_iters):\n",
        "        # gradient를 구해준다 c2\n",
        "        temp = (alpha / m) * np.matmul(X.T, (hx - Y))\n",
        "        c2 = Cost(theta - temp, X, Y)\n",
        "        \n",
        "        # c1과 c2를 비교해 gradient 변화량이 더 작은걸 넣어준다.\n",
        "        if c1 > c2:\n",
        "            c1 = c2\n",
        "            theta = theta - temp\n",
        "    \n",
        "    return theta\n",
        "\n",
        "def Cost(theta, X, Y):\n",
        "    m = Y.size\n",
        "    hx = np.matmul(X, theta)\n",
        "    return np.sum(np.power(np.subtract(hx, Y), 2)) / (2 * m)  \n",
        "\n",
        "# csv 파일을 불러들이는 코드\n",
        "train_f = 'gdrive/My Drive/linear_regression_train.csv'\n",
        "\n",
        "X, Y = loadData(train_f)\n",
        "\n",
        "# train 부분\n",
        "# m에 데이터셋의 총 크기를 담는다.\n",
        "m = Y.size\n",
        "t = X\n",
        "X = np.hstack((np.matrix(np.ones(m).reshape(m, 1)), t))\n",
        "print(X)\n",
        "\n",
        "theta = np.matrix(np.ones(2).reshape(2, 1))\n",
        "theta = GradientDescent(X, Y, theta, 0.000001, 100)\n",
        "Cost(theta, X, Y)\n",
        "# print(theta)\n",
        "plt.scatter(np.array(t), np.array(Y), marker = 'x', color='r')\n",
        "plt.plot(np.array(t),np.array(np.matmul(X, theta)))\n",
        "plt.title(\"After Regression: slope = \" + str(theta))\n",
        "plt.show()\n",
        "# cost 함수 값 확인\n",
        "print(Cost(theta, X, Y))\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[[ 32.50234527  31.70700585]\n",
            " [ 53.42680403  68.77759598]\n",
            " [ 61.53035803  62.5623823 ]\n",
            " [ 47.47563963  71.54663223]\n",
            " [ 59.81320787  87.23092513]\n",
            " [ 55.14218841  78.21151827]\n",
            " [ 52.21179669  79.64197305]\n",
            " [ 39.29956669  59.17148932]\n",
            " [ 48.10504169  75.3312423 ]\n",
            " [ 52.55001444  71.30087989]\n",
            " [ 45.41973014  55.16567715]\n",
            " [ 54.35163488  82.47884676]\n",
            " [ 44.1640495   62.00892325]\n",
            " [ 58.16847072  75.39287043]\n",
            " [ 56.72720806  81.43619216]\n",
            " [ 48.95588857  60.72360244]\n",
            " [ 44.68719623  82.89250373]\n",
            " [ 60.29732685  97.37989686]\n",
            " [ 45.61864377  48.84715332]\n",
            " [ 38.81681754  56.87721319]\n",
            " [ 66.18981661  83.87856466]\n",
            " [ 65.41605175 118.5912173 ]\n",
            " [ 47.48120861  57.25181946]\n",
            " [ 41.57564262  51.39174408]\n",
            " [ 51.84518691  75.38065167]\n",
            " [ 59.37082201  74.76556403]\n",
            " [ 57.31000344  95.45505292]\n",
            " [ 63.61556125  95.22936602]\n",
            " [ 46.73761941  79.05240617]\n",
            " [ 50.55676015  83.43207142]\n",
            " [ 52.22399609  63.35879032]\n",
            " [ 35.56783005  41.4128853 ]\n",
            " [ 42.43647694  76.61734128]\n",
            " [ 58.16454011  96.76956643]\n",
            " [ 57.50444762  74.08413012]\n",
            " [ 45.44053073  66.58814441]\n",
            " [ 61.89622268  77.76848242]\n",
            " [ 33.09383174  50.71958891]\n",
            " [ 36.43600951  62.12457082]\n",
            " [ 37.67565486  60.81024665]\n",
            " [ 44.55560838  52.68298337]\n",
            " [ 43.31828263  58.56982472]\n",
            " [ 50.07314563  82.90598149]\n",
            " [ 43.87061265  61.4247098 ]\n",
            " [ 62.99748075 115.2441528 ]\n",
            " [ 32.66904376  45.57058882]\n",
            " [ 40.16689901  54.0840548 ]\n",
            " [ 53.57507753  87.99445276]\n",
            " [ 33.86421497  52.72549438]\n",
            " [ 64.70713867  93.57611869]\n",
            " [ 38.11982403  80.16627545]\n",
            " [ 44.50253806  65.10171157]\n",
            " [ 40.59953838  65.56230126]\n",
            " [ 41.72067636  65.28088692]\n",
            " [ 51.08863468  73.43464155]\n",
            " [ 55.0780959   71.13972786]\n",
            " [ 41.37772653  79.10282968]\n",
            " [ 62.49469743  86.52053844]\n",
            " [ 49.20388754  84.74269781]\n",
            " [ 41.10268519  59.35885025]\n",
            " [ 41.18201611  61.68403752]\n",
            " [ 50.18638949  69.84760416]\n",
            " [ 52.37844622  86.09829121]\n",
            " [ 50.13548549  59.10883927]\n",
            " [ 33.64470601  69.89968164]\n",
            " [ 39.55790122  44.86249071]\n",
            " [ 56.13038882  85.49806778]\n",
            " [ 57.36205213  95.53668685]\n",
            " [ 60.26921439  70.25193442]\n",
            " [ 35.67809389  52.72173496]\n",
            " [ 31.588117    50.39267014]\n",
            " [ 53.66093226  63.64239878]\n",
            " [ 46.68222865  72.24725107]\n",
            " [ 43.10782022  57.81251298]\n",
            " [ 70.34607562 104.25710159]\n",
            " [ 44.49285588  86.64202032]\n",
            " [ 57.5045333   91.486778  ]\n",
            " [ 36.93007661  55.23166089]\n",
            " [ 55.80573336  79.55043668]\n",
            " [ 38.95476907  44.84712424]\n",
            " [ 56.9012147   80.20752314]\n",
            " [ 56.86890066  83.14274979]\n",
            " [ 34.3331247   55.72348926]\n",
            " [ 59.04974121  77.63418251]\n",
            " [ 57.78822399  99.05141484]\n",
            " [ 54.28232871  79.12064627]\n",
            " [ 51.0887199   69.58889785]\n",
            " [ 50.28283635  69.51050331]\n",
            " [ 44.21174175  73.68756432]\n",
            " [ 38.00548801  61.36690454]\n",
            " [ 32.94047994  67.17065577]\n",
            " [ 53.69163957  85.66820315]\n",
            " [ 68.76573427 114.85387123]\n",
            " [ 46.2309665   90.12357207]\n",
            " [ 68.31936082  97.91982104]\n",
            " [ 50.03017434  81.53699078]\n",
            " [ 49.23976534  72.11183247]\n",
            " [ 50.03957594  85.23200734]\n",
            " [ 48.14985889  66.22495789]\n",
            " [ 25.12848465  53.45439421]]\n",
            "(100, 2)\n",
            "[[ 1.         32.50234527]\n",
            " [ 1.         53.42680403]\n",
            " [ 1.         61.53035803]\n",
            " [ 1.         47.47563963]\n",
            " [ 1.         59.81320787]\n",
            " [ 1.         55.14218841]\n",
            " [ 1.         52.21179669]\n",
            " [ 1.         39.29956669]\n",
            " [ 1.         48.10504169]\n",
            " [ 1.         52.55001444]\n",
            " [ 1.         45.41973014]\n",
            " [ 1.         54.35163488]\n",
            " [ 1.         44.1640495 ]\n",
            " [ 1.         58.16847072]\n",
            " [ 1.         56.72720806]\n",
            " [ 1.         48.95588857]\n",
            " [ 1.         44.68719623]\n",
            " [ 1.         60.29732685]\n",
            " [ 1.         45.61864377]\n",
            " [ 1.         38.81681754]\n",
            " [ 1.         66.18981661]\n",
            " [ 1.         65.41605175]\n",
            " [ 1.         47.48120861]\n",
            " [ 1.         41.57564262]\n",
            " [ 1.         51.84518691]\n",
            " [ 1.         59.37082201]\n",
            " [ 1.         57.31000344]\n",
            " [ 1.         63.61556125]\n",
            " [ 1.         46.73761941]\n",
            " [ 1.         50.55676015]\n",
            " [ 1.         52.22399609]\n",
            " [ 1.         35.56783005]\n",
            " [ 1.         42.43647694]\n",
            " [ 1.         58.16454011]\n",
            " [ 1.         57.50444762]\n",
            " [ 1.         45.44053073]\n",
            " [ 1.         61.89622268]\n",
            " [ 1.         33.09383174]\n",
            " [ 1.         36.43600951]\n",
            " [ 1.         37.67565486]\n",
            " [ 1.         44.55560838]\n",
            " [ 1.         43.31828263]\n",
            " [ 1.         50.07314563]\n",
            " [ 1.         43.87061265]\n",
            " [ 1.         62.99748075]\n",
            " [ 1.         32.66904376]\n",
            " [ 1.         40.16689901]\n",
            " [ 1.         53.57507753]\n",
            " [ 1.         33.86421497]\n",
            " [ 1.         64.70713867]\n",
            " [ 1.         38.11982403]\n",
            " [ 1.         44.50253806]\n",
            " [ 1.         40.59953838]\n",
            " [ 1.         41.72067636]\n",
            " [ 1.         51.08863468]\n",
            " [ 1.         55.0780959 ]\n",
            " [ 1.         41.37772653]\n",
            " [ 1.         62.49469743]\n",
            " [ 1.         49.20388754]\n",
            " [ 1.         41.10268519]\n",
            " [ 1.         41.18201611]\n",
            " [ 1.         50.18638949]\n",
            " [ 1.         52.37844622]\n",
            " [ 1.         50.13548549]\n",
            " [ 1.         33.64470601]\n",
            " [ 1.         39.55790122]\n",
            " [ 1.         56.13038882]\n",
            " [ 1.         57.36205213]\n",
            " [ 1.         60.26921439]\n",
            " [ 1.         35.67809389]\n",
            " [ 1.         31.588117  ]\n",
            " [ 1.         53.66093226]\n",
            " [ 1.         46.68222865]\n",
            " [ 1.         43.10782022]\n",
            " [ 1.         70.34607562]\n",
            " [ 1.         44.49285588]\n",
            " [ 1.         57.5045333 ]\n",
            " [ 1.         36.93007661]\n",
            " [ 1.         55.80573336]\n",
            " [ 1.         38.95476907]\n",
            " [ 1.         56.9012147 ]\n",
            " [ 1.         56.86890066]\n",
            " [ 1.         34.3331247 ]\n",
            " [ 1.         59.04974121]\n",
            " [ 1.         57.78822399]\n",
            " [ 1.         54.28232871]\n",
            " [ 1.         51.0887199 ]\n",
            " [ 1.         50.28283635]\n",
            " [ 1.         44.21174175]\n",
            " [ 1.         38.00548801]\n",
            " [ 1.         32.94047994]\n",
            " [ 1.         53.69163957]\n",
            " [ 1.         68.76573427]\n",
            " [ 1.         46.2309665 ]\n",
            " [ 1.         68.31936082]\n",
            " [ 1.         50.03017434]\n",
            " [ 1.         49.23976534]\n",
            " [ 1.         50.03957594]\n",
            " [ 1.         48.14985889]\n",
            " [ 1.         25.12848465]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFnCAYAAAB3ijqPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIg0gAGZiIhIAxiQiYiINOD/AVyEAKRxgfhEAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f28ca8547f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "204.50666115574757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mORdoeq7U-Ps",
        "colab_type": "code",
        "outputId": "cf4b2518-ae4d-4115-853a-d62fcaf73779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3590
        }
      },
      "cell_type": "code",
      "source": [
        "# Softmax Classifier\n",
        "import numpy\n",
        " \n",
        "def sigmoid(x):\n",
        "    return 1. / (1 + numpy.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    e = numpy.exp(x - numpy.max(x)) \n",
        "    if e.ndim == 1:\n",
        "        return e / numpy.sum(e, axis=0)\n",
        "    else:  \n",
        "        return e / numpy.array([numpy.sum(e, axis=1)]).T  # ndim = 2\n",
        "\n",
        "class LogisticRegression(object):\n",
        "    def __init__(self, input, label, n_in, n_out):\n",
        "        self.x = input\n",
        "        self.y = label\n",
        "        self.W = numpy.zeros((n_in, n_out))  # initialize W 0\n",
        "        self.b = numpy.zeros(n_out)          # initialize bias 0\n",
        "\n",
        "        # self.params = [self.W, self.b]\n",
        "\n",
        "    def train(self, lr=0.1, input=None, L2_reg=0.00):\n",
        "        if input is not None:\n",
        "            self.x = input\n",
        "\n",
        "        # p_y_given_x = sigmoid(numpy.dot(self.x, self.W) + self.b)\n",
        "        p_y_given_x = softmax(numpy.dot(self.x, self.W) + self.b)\n",
        "        d_y = self.y - p_y_given_x\n",
        "        \n",
        "        self.W += lr * numpy.dot(self.x.T, d_y) - lr * L2_reg * self.W\n",
        "        self.b += lr * numpy.mean(d_y, axis=0)\n",
        "        \n",
        "        # cost = self.negative_log_likelihood()\n",
        "        # return cost\n",
        "\n",
        "    def negative_log_likelihood(self):\n",
        "        # sigmoid_activation = sigmoid(numpy.dot(self.x, self.W) + self.b)\n",
        "        sigmoid_activation = softmax(numpy.dot(self.x, self.W) + self.b)\n",
        "\n",
        "        cross_entropy = - numpy.mean(\n",
        "            numpy.sum(self.y * numpy.log(sigmoid_activation) +\n",
        "            (1 - self.y) * numpy.log(1 - sigmoid_activation),\n",
        "                      axis=1))\n",
        "\n",
        "        return cross_entropy\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        # return sigmoid(numpy.dot(x, self.W) + self.b)\n",
        "        return softmax(numpy.dot(x, self.W) + self.b)\n",
        "\n",
        "\n",
        "# training data\n",
        "x = numpy.array([[1, 0, 0],\n",
        "                 [0, 1, 0],\n",
        "                 [0, 0, 1]])\n",
        "y = numpy.array([[1, 0, 0],\n",
        "                 [0, 1, 0],\n",
        "                 [0, 0, 1]])\n",
        "\n",
        "# construct LogisticRegression\n",
        "classifier = LogisticRegression(input=x, label=y, n_in=3, n_out=3)\n",
        "\n",
        "# train\n",
        "for epoch in range(n_epochs):\n",
        "  classifier.train(lr=learning_rate)\n",
        "  cost = classifier.negative_log_likelihood()\n",
        "  print('Training epoch %d, cost is ' % epoch, cost)\n",
        "  learning_rate *= 0.95\n",
        "\n",
        "# test\n",
        "x = numpy.array([1, 0, 0])\n",
        "print('predict : ', classifier.predict(x))\n",
        "x = numpy.array([0, 1, 0])\n",
        "print('predict :', classifier.predict(x))\n",
        "x = numpy.array([0, 0, 1])\n",
        "print('predict :', classifier.predict(x))\n",
        "    "
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch 0, cost is  1.8995508752660737\n",
            "Training epoch 1, cost is  1.8901060820930937\n",
            "Training epoch 2, cost is  1.8811762090490756\n",
            "Training epoch 3, cost is  1.8727313731152908\n",
            "Training epoch 4, cost is  1.8647435821982998\n",
            "Training epoch 5, cost is  1.8571866034289393\n",
            "Training epoch 6, cost is  1.850035841338275\n",
            "Training epoch 7, cost is  1.8432682251641357\n",
            "Training epoch 8, cost is  1.8368621045900317\n",
            "Training epoch 9, cost is  1.8307971532656824\n",
            "Training epoch 10, cost is  1.825054279504399\n",
            "Training epoch 11, cost is  1.8196155435966974\n",
            "Training epoch 12, cost is  1.8144640812215667\n",
            "Training epoch 13, cost is  1.8095840324764652\n",
            "Training epoch 14, cost is  1.8049604760844407\n",
            "Training epoch 15, cost is  1.8005793683715787\n",
            "Training epoch 16, cost is  1.7964274866404875\n",
            "Training epoch 17, cost is  1.792492376595594\n",
            "Training epoch 18, cost is  1.7887623035039697\n",
            "Training epoch 19, cost is  1.7852262068010996\n",
            "Training epoch 20, cost is  1.7818736578747778\n",
            "Training epoch 21, cost is  1.7786948207821502\n",
            "Training epoch 22, cost is  1.7756804156749773\n",
            "Training epoch 23, cost is  1.7728216847266516\n",
            "Training epoch 24, cost is  1.7701103603713875\n",
            "Training epoch 25, cost is  1.7675386356814855\n",
            "Training epoch 26, cost is  1.765099136722803\n",
            "Training epoch 27, cost is  1.7627848967415132\n",
            "Training epoch 28, cost is  1.760589332047165\n",
            "Training epoch 29, cost is  1.7585062194679253\n",
            "Training epoch 30, cost is  1.7565296752638535\n",
            "Training epoch 31, cost is  1.7546541353931737\n",
            "Training epoch 32, cost is  1.7528743370348518\n",
            "Training epoch 33, cost is  1.7511853012784175\n",
            "Training epoch 34, cost is  1.7495823168989644\n",
            "Training epoch 35, cost is  1.7480609251416563\n",
            "Training epoch 36, cost is  1.7466169054459229\n",
            "Training epoch 37, cost is  1.7452462620449072\n",
            "Training epoch 38, cost is  1.7439452113806357\n",
            "Training epoch 39, cost is  1.7427101702799017\n",
            "Training epoch 40, cost is  1.741537744839972\n",
            "Training epoch 41, cost is  1.7404247199770542\n",
            "Training epoch 42, cost is  1.739368049593902\n",
            "Training epoch 43, cost is  1.7383648473261857\n",
            "Training epoch 44, cost is  1.7374123778301538\n",
            "Training epoch 45, cost is  1.736508048576847\n",
            "Training epoch 46, cost is  1.7356494021205922\n",
            "Training epoch 47, cost is  1.7348341088118167\n",
            "Training epoch 48, cost is  1.734059959926307\n",
            "Training epoch 49, cost is  1.7333248611850152\n",
            "Training epoch 50, cost is  1.7326268266402647\n",
            "Training epoch 51, cost is  1.7319639729059209\n",
            "Training epoch 52, cost is  1.731334513710556\n",
            "Training epoch 53, cost is  1.7307367547541073\n",
            "Training epoch 54, cost is  1.7301690888498003\n",
            "Training epoch 55, cost is  1.7296299913343225\n",
            "Training epoch 56, cost is  1.7291180157303698\n",
            "Training epoch 57, cost is  1.7286317896466963\n",
            "Training epoch 58, cost is  1.7281700109017866\n",
            "Training epoch 59, cost is  1.7277314438581428\n",
            "Training epoch 60, cost is  1.727314915955022\n",
            "Training epoch 61, cost is  1.7269193144282056\n",
            "Training epoch 62, cost is  1.7265435832061298\n",
            "Training epoch 63, cost is  1.7261867199723424\n",
            "Training epoch 64, cost is  1.7258477733848814\n",
            "Training epoch 65, cost is  1.7255258404437583\n",
            "Training epoch 66, cost is  1.725220063998239\n",
            "Training epoch 67, cost is  1.7249296303861492\n",
            "Training epoch 68, cost is  1.7246537671978726\n",
            "Training epoch 69, cost is  1.7243917411581584\n",
            "Training epoch 70, cost is  1.7241428561192647\n",
            "Training epoch 71, cost is  1.7239064511593327\n",
            "Training epoch 72, cost is  1.7236818987802682\n",
            "Training epoch 73, cost is  1.7234686031997006\n",
            "Training epoch 74, cost is  1.7232659987319519\n",
            "Training epoch 75, cost is  1.7230735482531934\n",
            "Training epoch 76, cost is  1.722890741746277\n",
            "Training epoch 77, cost is  1.7227170949209658\n",
            "Training epoch 78, cost is  1.722552147905537\n",
            "Training epoch 79, cost is  1.7223954640059569\n",
            "Training epoch 80, cost is  1.7222466285290376\n",
            "Training epoch 81, cost is  1.7221052476661896\n",
            "Training epoch 82, cost is  1.7219709474345646\n",
            "Training epoch 83, cost is  1.721843372672575\n",
            "Training epoch 84, cost is  1.7217221860869152\n",
            "Training epoch 85, cost is  1.7216070673484012\n",
            "Training epoch 86, cost is  1.7214977122340642\n",
            "Training epoch 87, cost is  1.7213938318130844\n",
            "Training epoch 88, cost is  1.721295151674287\n",
            "Training epoch 89, cost is  1.7212014111930245\n",
            "Training epoch 90, cost is  1.7211123628354237\n",
            "Training epoch 91, cost is  1.721027771498033\n",
            "Training epoch 92, cost is  1.720947413881068\n",
            "Training epoch 93, cost is  1.720871077893495\n",
            "Training epoch 94, cost is  1.7207985620883246\n",
            "Training epoch 95, cost is  1.7207296751265624\n",
            "Training epoch 96, cost is  1.7206642352683292\n",
            "Training epoch 97, cost is  1.7206020698897717\n",
            "Training epoch 98, cost is  1.7205430150244265\n",
            "Training epoch 99, cost is  1.7204869149277997\n",
            "Training epoch 100, cost is  1.72043362166396\n",
            "Training epoch 101, cost is  1.7203829947130285\n",
            "Training epoch 102, cost is  1.7203349005985045\n",
            "Training epoch 103, cost is  1.7202892125333928\n",
            "Training epoch 104, cost is  1.7202458100842073\n",
            "Training epoch 105, cost is  1.7202045788519085\n",
            "Training epoch 106, cost is  1.72016541016894\n",
            "Training epoch 107, cost is  1.7201282008115293\n",
            "Training epoch 108, cost is  1.7200928527264807\n",
            "Training epoch 109, cost is  1.7200592727717356\n",
            "Training epoch 110, cost is  1.7200273724699848\n",
            "Training epoch 111, cost is  1.7199970677746894\n",
            "Training epoch 112, cost is  1.7199682788478654\n",
            "Training epoch 113, cost is  1.7199409298490513\n",
            "Training epoch 114, cost is  1.719914948734882\n",
            "Training epoch 115, cost is  1.7198902670687406\n",
            "Training epoch 116, cost is  1.7198668198399727\n",
            "Training epoch 117, cost is  1.7198445452921873\n",
            "Training epoch 118, cost is  1.7198233847601792\n",
            "Training epoch 119, cost is  1.7198032825150407\n",
            "Training epoch 120, cost is  1.719784185617052\n",
            "Training epoch 121, cost is  1.7197660437759519\n",
            "Training epoch 122, cost is  1.719748809218227\n",
            "Training epoch 123, cost is  1.7197324365610551\n",
            "Training epoch 124, cost is  1.7197168826925722\n",
            "Training epoch 125, cost is  1.7197021066581504\n",
            "Training epoch 126, cost is  1.7196880695523744\n",
            "Training epoch 127, cost is  1.7196747344164371\n",
            "Training epoch 128, cost is  1.7196620661406758\n",
            "Training epoch 129, cost is  1.7196500313720045\n",
            "Training epoch 130, cost is  1.7196385984259706\n",
            "Training epoch 131, cost is  1.7196277372032318\n",
            "Training epoch 132, cost is  1.7196174191102145\n",
            "Training epoch 133, cost is  1.7196076169837449\n",
            "Training epoch 134, cost is  1.7195983050194614\n",
            "Training epoch 135, cost is  1.7195894587038076\n",
            "Training epoch 136, cost is  1.7195810547494361\n",
            "Training epoch 137, cost is  1.7195730710338477\n",
            "Training epoch 138, cost is  1.7195654865410976\n",
            "Training epoch 139, cost is  1.7195582813064323\n",
            "Training epoch 140, cost is  1.7195514363636857\n",
            "Training epoch 141, cost is  1.7195449336953186\n",
            "Training epoch 142, cost is  1.7195387561849558\n",
            "Training epoch 143, cost is  1.719532887572301\n",
            "Training epoch 144, cost is  1.7195273124103043\n",
            "Training epoch 145, cost is  1.7195220160244802\n",
            "Training epoch 146, cost is  1.7195169844742584\n",
            "Training epoch 147, cost is  1.7195122045162685\n",
            "Training epoch 148, cost is  1.7195076635694628\n",
            "Training epoch 149, cost is  1.719503349681988\n",
            "Training epoch 150, cost is  1.719499251499708\n",
            "Training epoch 151, cost is  1.7194953582363077\n",
            "Training epoch 152, cost is  1.719491659644891\n",
            "Training epoch 153, cost is  1.719488145991\n",
            "Training epoch 154, cost is  1.719484808026982\n",
            "Training epoch 155, cost is  1.7194816369676442\n",
            "Training epoch 156, cost is  1.7194786244671203\n",
            "Training epoch 157, cost is  1.7194757625968997\n",
            "Training epoch 158, cost is  1.7194730438249526\n",
            "Training epoch 159, cost is  1.719470460995901\n",
            "Training epoch 160, cost is  1.719468007312182\n",
            "Training epoch 161, cost is  1.7194656763161487\n",
            "Training epoch 162, cost is  1.7194634618730777\n",
            "Training epoch 163, cost is  1.719461358155011\n",
            "Training epoch 164, cost is  1.7194593596254213\n",
            "Training epoch 165, cost is  1.719457461024634\n",
            "Training epoch 166, cost is  1.7194556573559818\n",
            "Training epoch 167, cost is  1.7194539438726537\n",
            "Training epoch 168, cost is  1.7194523160652002\n",
            "Training epoch 169, cost is  1.7194507696496597\n",
            "Training epoch 170, cost is  1.7194493005562872\n",
            "Training epoch 171, cost is  1.7194479049188385\n",
            "Training epoch 172, cost is  1.7194465790643942\n",
            "Training epoch 173, cost is  1.719445319503695\n",
            "Training epoch 174, cost is  1.7194441229219528\n",
            "Training epoch 175, cost is  1.7194429861701306\n",
            "Training epoch 176, cost is  1.7194419062566506\n",
            "Training epoch 177, cost is  1.719440880339523\n",
            "Training epoch 178, cost is  1.7194399057188638\n",
            "Training epoch 179, cost is  1.7194389798297902\n",
            "Training epoch 180, cost is  1.7194381002356682\n",
            "Training epoch 181, cost is  1.7194372646217027\n",
            "Training epoch 182, cost is  1.7194364707888414\n",
            "Training epoch 183, cost is  1.7194357166479903\n",
            "Training epoch 184, cost is  1.7194350002145111\n",
            "Training epoch 185, cost is  1.7194343196030057\n",
            "Training epoch 186, cost is  1.719433673022344\n",
            "Training epoch 187, cost is  1.7194330587709585\n",
            "Training epoch 188, cost is  1.7194324752323624\n",
            "Training epoch 189, cost is  1.7194319208708935\n",
            "Training epoch 190, cost is  1.7194313942276773\n",
            "Training epoch 191, cost is  1.7194308939167826\n",
            "Training epoch 192, cost is  1.7194304186215783\n",
            "Training epoch 193, cost is  1.7194299670912663\n",
            "Training epoch 194, cost is  1.7194295381375877\n",
            "Training epoch 195, cost is  1.7194291306317002\n",
            "Training epoch 196, cost is  1.7194287435012037\n",
            "Training epoch 197, cost is  1.7194283757273192\n",
            "Training epoch 198, cost is  1.7194280263422075\n",
            "Training epoch 199, cost is  1.7194276944264224\n",
            "predict :  [0.37762459 0.31118771 0.31118771]\n",
            "predict : [0.31118771 0.37762459 0.31118771]\n",
            "predict : [0.31118771 0.31118771 0.37762459]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YkEyqOtkVDDH",
        "colab_type": "code",
        "outputId": "90ca9cd3-06e0-45e3-8fa0-46ce0dc21b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        }
      },
      "cell_type": "code",
      "source": [
        "# Neural Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters(사용자가 지정해주는 파라미터)\n",
        "#input_size는 28*28 이미지 -> 784차원의 벡터로 표현 가능\n",
        "input_size = 784\n",
        "# hidden_size는 중간 hidden layer의 depth\n",
        "hidden_size = 500\n",
        "# 최종 class(10개)\n",
        "num_classes = 10\n",
        "# 몇번이나 데이터셋을 돌릴것인가? \n",
        "num_epochs = 5\n",
        "# 한번에 학습시키는데 몇장의 이미지를 넣을것인가?\n",
        "batch_size = 100\n",
        "# 코스트 함수에서의 알파값을 의미\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "# Fully connected neural network with one hidden layer\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # neural net 설계\n",
        "        # nn.Linear는 fully cunnected layer를 의미한다\n",
        "        # nn.Linear의 인풋 파라미터로는 입력depth와 출력 depth\n",
        "        # 만을 입력해주면 알아서 해준다\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
        "        # relu 함수를 통해 다음 레이어로 feature를 전달해준다.\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    # 앞으로 전달\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "# Cross-Entropy을 loss로 사용\n",
        "# optimizer는 Adam이라는 optimizer로 성능이 좋아 기본으로 사용된다.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Train the model\n",
        "# len 함수를 통해 데이터셋의 총 갯수를 알수 있다\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    # enumerate 함수는 train_loader에서 i와 같은 카운트를 할 수 있도록 도와줌\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # optimizer의 gradient를 초기화해준다\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation을 해준다\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "# 정확한 accuracy 측정을 위해 test셋을 이용해 test를 진행시킨다\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.3152\n",
            "Epoch [1/5], Step [200/600], Loss: 0.1460\n",
            "Epoch [1/5], Step [300/600], Loss: 0.2947\n",
            "Epoch [1/5], Step [400/600], Loss: 0.1806\n",
            "Epoch [1/5], Step [500/600], Loss: 0.1333\n",
            "Epoch [1/5], Step [600/600], Loss: 0.2735\n",
            "Epoch [2/5], Step [100/600], Loss: 0.2332\n",
            "Epoch [2/5], Step [200/600], Loss: 0.1389\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0905\n",
            "Epoch [2/5], Step [400/600], Loss: 0.1161\n",
            "Epoch [2/5], Step [500/600], Loss: 0.1753\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0467\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0737\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0900\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0348\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0464\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0709\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0492\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0489\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0574\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0329\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0758\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0162\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0458\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0288\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0254\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0178\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0552\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0196\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0594\n",
            "Accuracy of the network on the 10000 test images: 97.96 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ot6YuoMFVDNc",
        "colab_type": "code",
        "outputId": "e7cd46ab-593d-4683-c2fc-b29ccf251a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        }
      },
      "cell_type": "code",
      "source": [
        "# CNN \n",
        "# neural net과 마찬가지로 혼자서 분석해보자!\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper parameters\n",
        "num_epochs = 5\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "# Convolutional neural network (two convolutional layers)\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # nn.Sequential 모듈을 이용해 간단하게 표현해 줄 수 있다\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Linear(7*7*32, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "model = ConvNet(num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.2512\n",
            "Epoch [1/5], Step [200/600], Loss: 0.1171\n",
            "Epoch [1/5], Step [300/600], Loss: 0.0788\n",
            "Epoch [1/5], Step [400/600], Loss: 0.0769\n",
            "Epoch [1/5], Step [500/600], Loss: 0.0181\n",
            "Epoch [1/5], Step [600/600], Loss: 0.0281\n",
            "Epoch [2/5], Step [100/600], Loss: 0.0779\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0299\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0539\n",
            "Epoch [2/5], Step [400/600], Loss: 0.0143\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0116\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0337\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0334\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0297\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0156\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0257\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0255\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0133\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0249\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0029\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0267\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0207\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0674\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0192\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0097\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0022\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0173\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0506\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0401\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0032\n",
            "Test Accuracy of the model on the 10000 test images: 98.97 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ybzgg8GJIAYj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GAN\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "latent_size = 64\n",
        "hidden_size = 256\n",
        "image_size = 784\n",
        "num_epochs = 200\n",
        "batch_size = 100\n",
        "sample_dir = 'samples'\n",
        "\n",
        "# Create a directory if not exists\n",
        "if not os.path.exists(sample_dir):\n",
        "    os.makedirs(sample_dir)\n",
        "\n",
        "# Image processing\n",
        "transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels\n",
        "                                     std=(0.5, 0.5, 0.5))])\n",
        "\n",
        "# MNIST dataset\n",
        "mnist = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                   train=True,\n",
        "                                   transform=transform,\n",
        "                                   download=True)\n",
        "\n",
        "# Data loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True)\n",
        "\n",
        "# Discriminator\n",
        "D = nn.Sequential(\n",
        "    nn.Linear(image_size, hidden_size),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Linear(hidden_size, hidden_size),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Linear(hidden_size, 1),\n",
        "    nn.Sigmoid())\n",
        "\n",
        "# Generator \n",
        "G = nn.Sequential(\n",
        "    nn.Linear(latent_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, image_size),\n",
        "    nn.Tanh())\n",
        "\n",
        "# Device setting\n",
        "D = D.to(device)\n",
        "G = G.to(device)\n",
        "\n",
        "# Binary cross entropy loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
        "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)\n",
        "\n",
        "def denorm(x):\n",
        "    out = (x + 1) / 2\n",
        "    return out.clamp(0, 1)\n",
        "\n",
        "def reset_grad():\n",
        "    d_optimizer.zero_grad()\n",
        "    g_optimizer.zero_grad()\n",
        "\n",
        "# Start training\n",
        "total_step = len(data_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, _) in enumerate(data_loader):\n",
        "        images = images.reshape(batch_size, -1).to(device)\n",
        "        \n",
        "        # Create the labels which are later used as input for the BCE loss\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                      Train the discriminator                       #\n",
        "        # ================================================================== #\n",
        "\n",
        "        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
        "        # Second term of the loss is always zero since real_labels == 1\n",
        "        outputs = D(images)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "        real_score = outputs\n",
        "        \n",
        "        # Compute BCELoss using fake images\n",
        "        # First term of the loss is always zero since fake_labels == 0\n",
        "        z = torch.randn(batch_size, latent_size).to(device)\n",
        "        fake_images = G(z)\n",
        "        outputs = D(fake_images)\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "        fake_score = outputs\n",
        "        \n",
        "        # Backprop and optimize\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        reset_grad()\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "        \n",
        "        # ================================================================== #\n",
        "        #                        Train the generator                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "        # Compute loss with fake images\n",
        "        z = torch.randn(batch_size, latent_size).to(device)\n",
        "        fake_images = G(z)\n",
        "        outputs = D(fake_images)\n",
        "        \n",
        "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
        "        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "        \n",
        "        # Backprop and optimize\n",
        "        reset_grad()\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "        \n",
        "        if (i+1) % 200 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
        "                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n",
        "                          real_score.mean().item(), fake_score.mean().item()))\n",
        "    \n",
        "    # Save real images\n",
        "    if (epoch+1) == 1:\n",
        "        images = images.reshape(images.size(0), 1, 28, 28)\n",
        "        save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))\n",
        "    \n",
        "    # Save sampled images\n",
        "    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
        "    save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}