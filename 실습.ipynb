{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gimikk/Colorization_for_BuddistArt/blob/master/%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SDL1okfxU4JM",
        "colab_type": "code",
        "outputId": "803cc1b7-3ad4-45bd-c0e8-80a10a8c7fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "cell_type": "code",
      "source": [
        "# pytorch 설치 명령어 입니다\n",
        "!pip3 install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 23kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x60b68000 @  0x7f424a06e2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 4.2MB/s \n",
            "\u001b[?25hInstalling collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-1.0.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pwye1IxtVYJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "47019562-f579-4894-8d3e-e56968b08595"
      },
      "cell_type": "code",
      "source": [
        "# 수치 미분\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# 파이썬은 파라미터로 함수도 받을 수 있다\n",
        "# f는 함수\n",
        "def numerical_diff(f, x):\n",
        "  h = 1e-4 # 0.0001 보통 많이 쓰는 h값\n",
        "  #return (f(x+h) - f(x)) / h     # 전향차분\n",
        "  return (f(x+h) - f(x-h))/ (2*h)# 중앙(중심)차분이 가장 정확함!\n",
        "\n",
        "def function1(x):\n",
        "  return 0.01*x**2 + 0.1*x\n",
        "\n",
        "# 편미분, x0, x1\n",
        "def function2(x):\n",
        "  return np.sum(x**2)\n",
        "\n",
        "# 수치적 gradient를 구하는법\n",
        "def numerical_gradient(f ,x):\n",
        "  h = 1e-4\n",
        "  grad = np.zeros_like(x) # x와 shape가 같은 array 생성\n",
        "  \n",
        "  for idx in range(x.size):\n",
        "    tmp_val = x[idx]\n",
        "    \n",
        "    x[idx] = tmp_val + h\n",
        "    fxh1 = f(x)\n",
        "    \n",
        "    x[idx] = tmp_val - h\n",
        "    fxh2 = f(x)\n",
        "    \n",
        "    grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "    x[idx] = tmp_val\n",
        "  \n",
        "  return grad\n",
        "\n",
        "x = np.arange(0.0, 20.0, 0.1)\n",
        "y = function1(x)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.plot(x, y)\n",
        "plt.show()\n",
        "\n",
        "print(numerical_diff(function1, 5))  # 해석적 미분값은 0.2\n",
        "print(numerical_diff(function1, 10)) # 해석적 미분값은 0.3\n",
        "\n",
        "print(numerical_diff(function2, np.array([5, 5]))) # 편미분\n",
        "\n",
        "print(numerical_gradient(function2, np.array([0.0, 2.0]))) # gradient 계산\n",
        "\n",
        "x = np.array([[1, 2], [3, 4]])\n",
        "y = np.array([[5, 6], [7, 8]])\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFYCAYAAABpkTT0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f28d8cd2550>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.1999999999990898\n",
            "0.2999999999986347\n",
            "19.99999999995339\n",
            "[0. 4.]\n",
            "[[19 22]\n",
            " [43 50]]\n",
            "[[19 22]\n",
            " [43 50]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3p2oLH-XU9ft",
        "colab_type": "code",
        "outputId": "5fb1e2fa-f775-46f7-b7c4-eee3ea80f5df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2174
        }
      },
      "cell_type": "code",
      "source": [
        "# Linear Regression\n",
        "# numpy는 numerical python의 약자로 행렬 계산에 특화된 package\n",
        "# matplotlib는 graph를 그려주는 등의 시각화 package\n",
        "# pandas는 데이터 분석, 데이터 처리를 위해 만들어진 package\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import io\n",
        "# 구글 드라이브에 csv 파일(엑셀)을 저장, 드라이브로 부터 읽어오는 함수\n",
        "# 구글 드라이브에 연결하기 위해 authorize 해야함!\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def loadData(fileName):\n",
        "  # np.loadtxt 함수로 csv 파일을 읽는다, delimiter는 파싱할 기준(,)\n",
        "  data = np.matrix(np.loadtxt(fname=fileName, delimiter=','))\n",
        "  # print를 찍어 데이터가 어떤지 확인해 보자\n",
        "  print(data)\n",
        "  # .shape를 이용해 shape를 알 수 있다.(모양)\n",
        "  print(data.shape)\n",
        "  # data행렬을 이해하고 그려보자!\n",
        "  return (data[:, 0], data[:, 1])\n",
        "\n",
        "# 편미분\n",
        "def function(x, w):\n",
        "  return np.sum(np.matmul(x,w))\n",
        "\n",
        "# 수치적 gradient를 구하는법\n",
        "# W 행렬을 업데이트\n",
        "def numerical_gradient(f ,X, W):\n",
        "  h = 1e-4\n",
        "  grad = np.zeros_like(W) # W와 shape가 같은 array 생성\n",
        "  \n",
        "  for idx in range(W.size):\n",
        "    tmp_val = W[idx]\n",
        "    \n",
        "    W[idx] = tmp_val + h\n",
        "    fxh1 = f(X, W)\n",
        "    \n",
        "    W[idx] = tmp_val - h\n",
        "    fxh2 = f(X, W)\n",
        "    \n",
        "    grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "    W[idx] = tmp_val\n",
        "  \n",
        "  return grad\n",
        "\n",
        "def GradientDescent(X, Y, W, alpha, num_iters):\n",
        "    m = np.size(Y)\n",
        "    # X의 shape는 (100, 1)\n",
        "    # W의 shape는 (1, 1)\n",
        "    # H_x는 (100, 1)\n",
        "    # H_x = Wx로 가설을 잡은 경우\n",
        "    # bias는 뺐습니다 여러분이 추가해보세요!\n",
        "    H_x = np.matmul(X, W)\n",
        "    cost1 = Cost(W, X, Y)\n",
        "    \n",
        "    for i in range(0, num_iters):\n",
        "        # gradient를 구해준다 cost2\n",
        "        #W_temp = (alpha / m) * np.matmul(X.T, (H_x - Y))\n",
        "        W_temp = (alpha/m) * numerical_gradient(function, X, W) \n",
        "        cost2 = Cost(W - W_temp, X, Y)\n",
        "        \n",
        "        # c1과 c2를 비교해 Cost가 더 작은걸 넣어준다.\n",
        "        if cost1 > cost2:\n",
        "            cost1 = cost2\n",
        "            W = W - W_temp\n",
        "    \n",
        "    return W\n",
        "\n",
        "# 수식을 참고하면서 Cost 함수를 만들어보자\n",
        "def Cost(W, X, Y):\n",
        "    m = Y.size\n",
        "    H_x = np.matmul(X, W)\n",
        "    return np.sum(np.power(np.subtract(H_x, Y), 2)) * (m/2)\n",
        "\n",
        "# csv 파일을 불러들이는 코드\n",
        "train_f = 'gdrive/My Drive/linear_regression_train.csv'\n",
        "X, Y = loadData(train_f)\n",
        "\n",
        "# array의 shape를 맞춰주는 작업\n",
        "m = X.size\n",
        "X = np.reshape(X, (m, 1))\n",
        "\n",
        "# np.reshape도 가능 하지만 .reshape를 통해서도 shape를 바꿀 수 있다.\n",
        "# 두가지 문법 다 알아둬야 나중에 코드 읽기 편함\n",
        "# W는 1로 초기화 해준다 -> 우리가 업데이트 해줘야할것!\n",
        "W = np.reshape(np.ones(1), (1, 1))\n",
        "W = GradientDescent(X, Y, W, 0.005, 100)\n",
        "\n",
        "Cost(W, X, Y)\n",
        "# Y데이터를 그래프에 그려줌\n",
        "plt.scatter(np.array(X), np.array(Y), marker = 'x', color='r')\n",
        "# 함수를 그려줌, X축 Y축 값 지정\n",
        "plt.plot(np.array(X),np.array(np.matmul(X, W)))\n",
        "plt.title(\"After Regression: slope = \" + str(W))\n",
        "plt.show()\n",
        "# cost 함수 값 확인\n",
        "print(Cost(W, X, Y))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[[ 32.50234527  31.70700585]\n",
            " [ 53.42680403  68.77759598]\n",
            " [ 61.53035803  62.5623823 ]\n",
            " [ 47.47563963  71.54663223]\n",
            " [ 59.81320787  87.23092513]\n",
            " [ 55.14218841  78.21151827]\n",
            " [ 52.21179669  79.64197305]\n",
            " [ 39.29956669  59.17148932]\n",
            " [ 48.10504169  75.3312423 ]\n",
            " [ 52.55001444  71.30087989]\n",
            " [ 45.41973014  55.16567715]\n",
            " [ 54.35163488  82.47884676]\n",
            " [ 44.1640495   62.00892325]\n",
            " [ 58.16847072  75.39287043]\n",
            " [ 56.72720806  81.43619216]\n",
            " [ 48.95588857  60.72360244]\n",
            " [ 44.68719623  82.89250373]\n",
            " [ 60.29732685  97.37989686]\n",
            " [ 45.61864377  48.84715332]\n",
            " [ 38.81681754  56.87721319]\n",
            " [ 66.18981661  83.87856466]\n",
            " [ 65.41605175 118.5912173 ]\n",
            " [ 47.48120861  57.25181946]\n",
            " [ 41.57564262  51.39174408]\n",
            " [ 51.84518691  75.38065167]\n",
            " [ 59.37082201  74.76556403]\n",
            " [ 57.31000344  95.45505292]\n",
            " [ 63.61556125  95.22936602]\n",
            " [ 46.73761941  79.05240617]\n",
            " [ 50.55676015  83.43207142]\n",
            " [ 52.22399609  63.35879032]\n",
            " [ 35.56783005  41.4128853 ]\n",
            " [ 42.43647694  76.61734128]\n",
            " [ 58.16454011  96.76956643]\n",
            " [ 57.50444762  74.08413012]\n",
            " [ 45.44053073  66.58814441]\n",
            " [ 61.89622268  77.76848242]\n",
            " [ 33.09383174  50.71958891]\n",
            " [ 36.43600951  62.12457082]\n",
            " [ 37.67565486  60.81024665]\n",
            " [ 44.55560838  52.68298337]\n",
            " [ 43.31828263  58.56982472]\n",
            " [ 50.07314563  82.90598149]\n",
            " [ 43.87061265  61.4247098 ]\n",
            " [ 62.99748075 115.2441528 ]\n",
            " [ 32.66904376  45.57058882]\n",
            " [ 40.16689901  54.0840548 ]\n",
            " [ 53.57507753  87.99445276]\n",
            " [ 33.86421497  52.72549438]\n",
            " [ 64.70713867  93.57611869]\n",
            " [ 38.11982403  80.16627545]\n",
            " [ 44.50253806  65.10171157]\n",
            " [ 40.59953838  65.56230126]\n",
            " [ 41.72067636  65.28088692]\n",
            " [ 51.08863468  73.43464155]\n",
            " [ 55.0780959   71.13972786]\n",
            " [ 41.37772653  79.10282968]\n",
            " [ 62.49469743  86.52053844]\n",
            " [ 49.20388754  84.74269781]\n",
            " [ 41.10268519  59.35885025]\n",
            " [ 41.18201611  61.68403752]\n",
            " [ 50.18638949  69.84760416]\n",
            " [ 52.37844622  86.09829121]\n",
            " [ 50.13548549  59.10883927]\n",
            " [ 33.64470601  69.89968164]\n",
            " [ 39.55790122  44.86249071]\n",
            " [ 56.13038882  85.49806778]\n",
            " [ 57.36205213  95.53668685]\n",
            " [ 60.26921439  70.25193442]\n",
            " [ 35.67809389  52.72173496]\n",
            " [ 31.588117    50.39267014]\n",
            " [ 53.66093226  63.64239878]\n",
            " [ 46.68222865  72.24725107]\n",
            " [ 43.10782022  57.81251298]\n",
            " [ 70.34607562 104.25710159]\n",
            " [ 44.49285588  86.64202032]\n",
            " [ 57.5045333   91.486778  ]\n",
            " [ 36.93007661  55.23166089]\n",
            " [ 55.80573336  79.55043668]\n",
            " [ 38.95476907  44.84712424]\n",
            " [ 56.9012147   80.20752314]\n",
            " [ 56.86890066  83.14274979]\n",
            " [ 34.3331247   55.72348926]\n",
            " [ 59.04974121  77.63418251]\n",
            " [ 57.78822399  99.05141484]\n",
            " [ 54.28232871  79.12064627]\n",
            " [ 51.0887199   69.58889785]\n",
            " [ 50.28283635  69.51050331]\n",
            " [ 44.21174175  73.68756432]\n",
            " [ 38.00548801  61.36690454]\n",
            " [ 32.94047994  67.17065577]\n",
            " [ 53.69163957  85.66820315]\n",
            " [ 68.76573427 114.85387123]\n",
            " [ 46.2309665   90.12357207]\n",
            " [ 68.31936082  97.91982104]\n",
            " [ 50.03017434  81.53699078]\n",
            " [ 49.23976534  72.11183247]\n",
            " [ 50.03957594  85.23200734]\n",
            " [ 48.14985889  66.22495789]\n",
            " [ 25.12848465  53.45439421]]\n",
            "(100, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFZCAYAAABJ+lxSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xd4VFX6B/BvSJlQEkoKiCLYyCJS\nRH4qSAsthUikCUJABHExoigqxUVFWXXVFREEC0V2iQoLgQQREVhgXV2MtEVAl6ZiQCCFJBNIZki5\nvz+SuWZCMvWWc2e+n+fhecjMnZkzJ5N57znnPe8NkCRJAhEREemqgd4NICIiIgZkIiIiITAgExER\nCYABmYiISAAMyERERAJgQCYiIhIAAzIZ2pgxYzB06NCrbn/mmWfQt29f/Pvf/8ahQ4fwv//9z6vX\nOXPmDGJiYhAfH4/4+HjExcVh8ODBeP3111FZWenVc6tl5syZ2Llzp6avGRMTg/Pnz2v6mo7Mnj0b\nvXr1wpw5c5CVlYUuXbogPj4eeXl5AICMjAzcfvvtyMzMrPPxn376KQYOHIj+/fsjLy8P8fHx6NKl\nC7KysrR8G+QnGJDJsI4fP46wsDC0bt0aBw8etLvv888/x+rVq9G7d2+kp6fj2LFjXr9eYGAgtm7d\niq1bt+LLL7/Ehg0bsH//fqxfv97r51bDG2+8gf79++vdDN3NmDEDr732GgCgc+fO2Lp1KyIjI/Hh\nhx9i69atuOGGG+p97AMPPIBVq1YBACIjI7F161Z07txZi2aTH2JAJsPauHEj4uPjkZSUhIyMDPn2\n8ePHo7KyEpMnT8bHH3+MzMxMvPnmm/joo48gSRLeffddxMXFITY2Fn/+859RUVEhP+7tt99GQkIC\nDhw44PT1mzRpgrvuugs//vgjAMBsNuPZZ59FXFwcBgwYgPT0dPnYDRs24J577sHQoUOxYcMGxMTE\nyLdPmzYNDz74IN544w0AwNq1axEfH4/+/ftjxowZsFgsAIDvvvsOw4YNQ2JiIhISEvDFF184vH38\n+PHyyC8rKwvDhg1DfHw8Ro0ahcOHD8uv/8QTT+C5555DXFwcEhMTceLECQDA9u3bMWfOnDrfe1pa\nGhISEhAfH4+RI0fKj6np73//OxITExEfH49HH30UFy9elNu1ePFi3H///ejZsyfmzp0r/w7279+P\nESNGYNCgQbj//vuRnZ3t9PfgqbvuugvvvfceGjdurNprELlFIjKg8vJyacCAAVJxcbFUUlIi9evX\nT7JarfL97du3l86dOydJkiSlpKRIGRkZkiRJ0saNG6UhQ4ZIZrNZKisrkx555BFp9erV8nGTJk2S\nKioqrnq97OxsqUOHDna3nT9/XoqPj5cyMzMlSZKkOXPmSDNnzpQqKiqk/Px8qW/fvtKxY8ekgoIC\nqXPnztKxY8ekiooK6amnnpLat28vSZIkpaenS127dpV+/vlnSZIkae/evVKPHj2k8+fPS5IkSc8/\n/7z0l7/8RZIkSRo+fLiUlZUlSZIk/fzzz9KMGTMc3m5735cuXZLuuusuad++fZIkSdLWrVulwYMH\nSxUVFVJ6errUpUsX6fDhw5IkSdK8efOkP/3pTw77vri4WOrevbtUXFwsSZIkbdmyRfrwww/t+v3g\nwYNSnz59pLy8PEmSJOnll1+WnnvuObldI0eOlEpKSqSSkhJp8ODB0vbt26Xi4mLp//7v/6Svv/5a\nkiRJ+uyzz6Rhw4Zd9fp79+6V4uLirvpna0NNs2bNktLT0yVJkqRvv/1WSklJueqYmp+PumRnZ0ux\nsbF2x3/77bcO+4jIExwhkyF9/fXX6NSpE5o0aYKGDRvizjvvxK5du5w+bteuXRgxYgTCwsIQFBSE\nUaNGYdu2bfL9ffv2RYMGdf9ZVFRUyGvI/fr1w/DhwzFu3Dh5DXvXrl2YMGECGjRogBYtWmDQoEHY\ntm0bDh06hHbt2qF9+/Zo0KABHnjgAbvnbdeuHdq1awcA2LlzJxITE9GyZUsAVVOmtvZFREQgIyMD\np06dQrt27fDWW285vN3m+++/R6tWrXDHHXcAAOLi4lBQUICzZ88CAG666SbcdtttAIBbb70V586d\nc9iHJpMJAQEBWL9+PfLy8pCQkIApU6bYHbN7927ExcUhIiICADBq1Ch888038v1DhgxBw4YN0bBh\nQ/Tu3RsHDx7E/v370bJlS9xzzz0AgKSkJPz666/47bff7J67e/fu8tJBzX+120BkNEF6N4DIExs2\nbMBXX32F7t27A6gKlkVFRYiLi3P4uOLiYqxYsQJr166VH9eiRQv5/qZNm9b7WNsaMgCcPHkS48aN\nw5AhQ+ye+8knn0RgYCAAwGq1Ij4+Hmaz2e55bcG2rtcsLi7G9u3b8fXXXwMAJElCWVkZAODVV1/F\ne++9h4ceegihoaGYMWMG4uPj673d5uLFiwgPD7d7zbCwMOTn58v/r/kebdPH9QkODsaqVavw/vvv\nY/HixYiJicGLL74oT8PbXjM6Olr+OTw8XH692u+5adOmyMnJgdlsRnZ2tl3bQ0JCcPHiRbRu3dph\nm4h8AQMyGU5RURG+++47ZGVlISQkBABQXl6Ovn374uLFi3YBtrbo6Gj0798fKSkpXrXh5ptvRmxs\nLJYsWYK5c+fKz71kyRK0b9/e7thdu3ahpKRE/jknJ8dh+4YNG4ZZs2ZddV9kZCSef/55PP/88/j6\n66/x+OOPo3fv3vXebhMREYHCwkL5Z0mSUFRUhIiICPz0008evf9bb70VixYtwpUrV7B8+XK8+OKL\nWLNmjV1ba75mYWEhIiMj5Z8LCgrk/xcVFaFp06aIjo7GjTfeiA0bNjh87X379sl9XtOIESM4SiZD\n45Q1Gc7nn3+Ou+++Ww7GABAUFIRevXph8+bNVx0fFBSE4uJiAMCAAQOQmZmJ0tJSAMCaNWuwceNG\nj9oxbdo0pKen4/Tp0wCA/v37y0GpvLwcr776Ko4ePYqOHTvi2LFjOH36NCorKx1mZffv3x/btm2T\nE6B27NiBDz/8EGVlZRg/frwczDt27IigoCBUVlbWeXvNaffOnTsjLy9PzkT//PPP0apVK1x33XUe\nve9jx47hiSeewJUrVxASEoLbbrsNAQEBdsf069cP27dvlwPvmjVr0LdvX/n+7du348qVKygpKZFn\nOrp06YLc3FwcOnQIAJCdnY1nn30WUq0L0nHKmnwVR8hkOBkZGXjwwQevun3QoEFYunQpJkyYYHf7\nwIED8eabbyI7OxuzZ8/GiRMnMGzYMADA9ddfj1deecWjdlx33XUYPnw4/vrXv2Lx4sV48skn8dJL\nL8nT5r1790ZMTAyCgoIwY8YMTJgwAZGRkRgzZky9JwEdO3bE1KlT5UzxiIgIvPTSSwgODsbIkSMx\nceJEAECDBg0wd+5chIWF1Xl7w4YN5eds1KgRFi5ciPnz56OkpAQtWrTAggULrgqitW3fvh07d+6U\ntwzZtG/fHtdddx2SkpIQHByMxo0b44UXXrA7pnPnznjkkUcwbtw4VFZWokOHDpg3b558/+23344J\nEybgl19+waBBg9CnTx80aNAAixYtwvz583H58mUEBwdj+vTpTtvpqcmTJ+Ps2bM4d+4cfv75Z7z3\n3nt4+umnMWjQIDz44IOYOXMmOnbsqMprE9UlQKp9+klEipMkSQ4sJ06cwNixY7F3716dW6WP8ePH\nY+TIkUhOTlb9tWbPno0777wTw4cPR1ZWFt59912sXr3arec4c+YMJkyYIBdZGT9+PKZNm4a77rpL\njSaTH+OUNZHKysvL0bt3b3kqdsuWLejatavOrSIi0TAgE6ksKCgIL774ImbNmoW4uDjs3bu3zqQk\nUseCBQvkAifff/+9XelMZz799FN5OcBWOvP7779Xq6nk5zhlTUREJACOkImIiATAgExERCQAXbc9\n5eYW6/nyumvevBEKCkqcH0heYT9rg/2sPvaxNtTs56iosHrv4whZR0FBgXo3wS+wn7XBflYf+1gb\nevUzAzIREZEAGJCJiIgEwIBMREQkAAZkIiIiAbgUkI8fP46BAwciLS0NAHDu3DlMnDgRKSkpmDhx\nInJzcwEAmzZtwogRIzBq1CisW7dOvVYTERH5GKcBuaSkBPPnz0ePHj3k2xYuXIj7778faWlpGDRo\nED766COUlJRgyZIlWLVqFVavXo2//e1vdtdDJSIiovo5DcghISFYtmwZoqOj5dtefPFF+RJzzZs3\nR2FhIQ4dOoROnTohLCwMoaGh6NatGw4cOKBey4mIiHyI08IgQUFBCAqyP6xRo0YAgIqKCnzyySd4\n7LHHkJeXhxYtWsjHtGjRQp7Krk/z5o38fl+do03ipBz2szbYz+ozZB9//DEwYgQQGvr7bRYLkJ4O\njBunX7sc0KOfPa7UVVFRgZkzZ+Luu+9Gjx498Nlnn9nd78o1K/y94kxUVJjfVyvTAvtZG+xn9Rmx\nj03r1yI8dQqsq/4O88q0qqBssSB8UgpMO7bBbC6FdeRovZtpR81+VqVS15w5c9C2bVtMmzYNABAd\nHW13SbOcnBy7aW4iIvI/1qRkWAcOhmnHNoRPSkGAuUgOxtaBg2FNSta7icLwKCBv2rQJwcHBeOKJ\nJ+TbunTpgsOHD8NsNuPy5cs4cOAAunfvrlhDiYjIgEJDYV6ZJgflyJvbyMFYHjETABeuh3zkyBG8\n/vrrOHv2LIKCgtCyZUvk5+fDZDKhSZMmAICbbroJ8+bNw9atW7FixQoEBAQgJSUFQ4cOdfjiRpt6\nUZoRp5+MiP2sDfaz+ozcxwHmIkTe3Eb+Oe9kNqTwpjq2qH56TVk7DchqMuoHSylG/uMyEvazNtjP\n6jNsH9dYM7YReYRsuDVkIiIip2oEY+vAwcg7mW23pgyLRe8WCoMBmYiIVGPanGm3ZiyFN7VbUzZt\nztS7icLweNsTERGRM9aRo2FGVba1PD1dnehl2pwp3JYnPTEgExGRquoMuqGhDMa1cMqaiIgAVBXx\nuGpN12Kpup1Ux4BMRERyRS27RKvqhKzw1CmGDMpGO8FgQCYiIp+rqGXEEwwGZCIi8rmKWkY8wWBS\nFxERVQkNRfH7K2CqUVGr+P0VhgvGAOQTDFsQtr0nkU8wOEImIqIqFgvCpk62uyls6mTjFu+oPsGo\nSeQTDAZkIiLyzYpaBjvBYEAmIiLfq6hlwBMMriETEZHPVdSqfYJx1ZqygO+JAZmIiAD4VkUtI55g\nMCATEZFPMtoJBteQiYiIBMCATEREJAAGZCIiIgEwIBMREQmAAZmIiEgADMhEREQCYEAmIiISAAMy\nERGRABiQiYiIBMCATEREJAAGZCIiIgEwIBMREQmAAZmIiEgADMhEREQCYEAmIiISAAMyERGRABiQ\niYiIBMCATEREJAAGZCIiIgEwIBMREQmAAZmIiEgADMhEREQ1ffwxYLHY32axwLR+raovy4BMRERU\nzbR+LZCSgvBJKb8HZYsF4ZNSEJ46RdWgzIBMRERUzZqUDCQmwrRjG8InpSDAXITwSSkw7dgG68DB\nVferJEi1ZyYiIjKa0FAgPR3We5Nh2rENppvbAACsAwfDvDKt6n6VcIRMRERUU2goit9fYXdT8fsr\nVA3GAAMyERGRPYsFYVMn290UNnXy1YleCmNAJiIisrFYgBEj5DXjvJPZsA4cLK8pqxmUGZCJiIiq\nmTZnAlu2yGvGUnhTmFemyUHZtDlTtddmUhcREVE168jRQHhDmPsM/n3NODQU5pVpMG3OrLpfJRwh\nExER1TRu3NUJXKGhqgZjgAGZiIhICAzIREREAmBAJiIiEoBLAfn48eMYOHAg0tLSAADnzp3D+PHj\nMXbsWEyfPh1XrlwBAGzatAkjRozAqFGjsG7dOvVaTURE5GOcBuSSkhLMnz8fPXr0kG9btGgRxo4d\ni08++QRt27bF+vXrUVJSgiVLlmDVqlVYvXo1/va3v6GwsFDVxhMREfkKpwE5JCQEy5YtQ3R0tHxb\nVlYWBgwYAACIjY3Fnj17cOjQIXTq1AlhYWEIDQ1Ft27dcODAAfVaTkQkINP6tbpcuo+Mz2lADgoK\nQmit9O/S0lKEhIQAACIiIpCbm4u8vDy0aNFCPqZFixbIzc1VuLlEROIyrV+L8NQpuly6j4zP68Ig\nkiS5dXtNzZs3QlBQoLdNMLSoqDC9m+AX2M/a8Pt+figF2LwRpi1bEDV1IvDJJ8DEicCObUBiIsIf\nSvH6AgV+38ca0aOfPQrIjRo1gsViQWhoKC5cuIDo6GhER0cjLy9PPiYnJwddu3Z1+DwFBSWevLzP\niIoKQ25usd7N8HnsZ22wn6u9v6rq+rlbtgDNmgGovnTf+6uA4rKqfx5iH2tDzX52FOg92vbUs2dP\nfPnllwCAbdu2oXfv3ujSpQsOHz4Ms9mMy5cv48CBA+jevbtnLSYiMiqdLt1Hxl+/dxqQjxw5gvHj\nx2Pjxo34+9//jvHjx2PatGnIyMjA2LFjUVhYiPvuuw+hoaF4+umnMXnyZDz00EN47LHHEBbGqRUi\n8jNqXrrv448NHXDU5Avr9wGSK4u9KvH3qRdOP2mD/awN9jPkAGC7dF/x+ysQNnWy/LN5ZZrHI2Vb\nwLF7nhqvZ166TPVay0JTsO8NNWVNRERXM23OtAsAUnhTWO+9D9b+A+0v3efBqNaalAwkJsrX5Q0w\nF9kFIGtSsgrvyECqr8hku0xi5M1tFDkR0hIvv0hEpBDryNEwozp4hoZWjWqnp8IaOwDmd5ZWjWBr\njmqrH+OS0FAgPR3We5OrgvvNbape00ABR3XV6/e2vgGMtX7PETIRkYKsI0fLAcCalFw1Ytv1T5g+\ny/B+VMuEMcfUXL/XAAMyEZGX6s3u3Zyp7DSqwQOOqmqtIeedzJb73S7RS2AMyEREXmg886mrs3sL\nC9EsYUBVdu/mTGVGtRYLMGKELgHHCNuJ6lq/r3kyJK/fC4wBmYjIQ6b1a9Fo1QpURET+nmyVk4MW\nPboh+OhhlHXsBOvAOEVGtabNmcCWLZoHHKNsJ7KOHA3z0mX2Mw/ViV5GyUDnticdcZuINtjP2vDL\nfq4xTVoREYnA/N+rFVZEROLi7v8g/Klpim2Ditq2Cbl9Bts/pnpqXLWAo+JWLlHpte2JAVlHfvkF\npgP2szb8tp9rBKya8o6cRMhXuxTdO6xbH9fxHn01GAP6BWRueyIi8kZoKIrfWQJTx1vsbg57MrVq\nuhS/b4OyHW9emabuqFZpBt9OZBRcQyYi8kZhIZr36WF3U801ZbtgbBMaapxgDLic3W2E5C+RMSAT\nEXnKYkGzYUMQmJ+HiohI5B09AevAwfLPRsnudcjF7URGSf4SGQMyEZGHTJsz5Wzqi3sOQIpqKWc+\nB+bnoWTiZGONhOvg6nYiuQgKS3t6jEldOvLbJBiNsZ+14a/9bFq/9uppaZUyn/XqY5ffo48kfzHL\n2g/56xeY1tjP2mA/q88IfRxgLkJkjeSvvJPZkMKb6tgi9/FqT0REZGws7ekVBmQiIgX4fYaxD9SS\n1hsDMhGRl5hh7Bu1pPXGwiBERF6yJiXDumGdPBqsXV7SHzKMa18LGoAxi6DoiAGZiMhb1YHHNmVr\nq2hlxAxjb9QZdI1WBEVHnLImMiC/X68UUXV5yZpYXpLcwYBMZDBcrxQUM4zJSwzIRAbDikgCYoYx\nKYABmchoqtcrbV/4kTe38elr0xoBM4xJCUzqIjIiXg5PKMwwJiVwhExkRFyvFI515GjjX2aRdMWA\nTGQ0XK+UMducfAkDMpHBcL2yCrPNyddwDZnIYLheWYXVscjXcIRMZEBcr4RfZJtzSt6/MCAT6YBf\ntArx4epYnJL3PwzIRBrjF62CfDjbvK4CMBgxQtEpeZ4YioUBmUhjrLTlmMtBwtezzeuYkseWLYpN\nyfPEUDwMyERa84O1T0+5EyRs2eZlHTvBvHT5VdnmjV+Yo9O7UJCKU/I8MRQPAzKRHnxk7VPpKU93\ngoR15GiUTJyM4KOHEZ76cFU7QkNhXrocZR07odGqFV6N8oSYzlVzSp4nhsJhQCbSgw+sfaoy5elm\nkLj88mtXB/DUhxF89LBXozwhpnPrmJJHYqKyU/I+cmLoKxiQibRmoLVPR6PE2qNZFCk05elOkFBp\nlCfCdG5dBWCQnq5sARgfODH0JQzIRBozSqUtp6PEzZl27UazZspMebobJNQY5QkwnWsdORrmpcvs\nX6+6Xealy7zfc26gE0N/wYBMpDHVv2gV4tIoUelg6EmQUGuUJ8B0rpoFYEQ5MRRirV4QDMhEOjBE\npS1XRokKB0O3g4Saozwfn84V4cRQiLV6gTAgE1H9HI0SawVDFBb+HgwnjrUPXC6OeNwNEqqN8vxk\nOlfvE0MR1upFwoBMpCPhp+scjBLlYNh/YFUAbdoU1nvvAwCYdu6AaeN6+TncGfG4EyTUGuU5DfS2\n92Yj0u/MSARYqxcJAzKRToSfrnM2SiwrqzpOkuSHWBOSUNEiAgDQ8MP3NBnx1BfAbe+h9nvyZqQu\nn3B8liHm78yIBFirFwUDMpFORJ+uczZKBFD1/13//H3bU+rDCLyYj4qISAQfPazbiEeJk526Ar11\n2Eihf2eG5ONr9e4IkKQap7cay80t1uulhRAVFeb3faAFofu5xijURqTpOtt+Y7u2VE9XW0eOrrf9\nxe8sQWTHW+Tb8k5mV+2j1Uqt0X3tayV7uy1Lr9+Z0J9lT6j5e/KCmv0cFRVW730cIRPpSfDpOmfr\nuabNmSh+Z4nd3cWvv42m9w+3u03zEY9Ka5O2kXXt35k1YYgwvzMjEWXrlSiC9G4AkV+rZ7pOlBGy\nI7Zp4YqISLvbI+7uioCyMpR17ISizC3yiCd8Uoq276v6ZMd0cxv5Jm9Odmzv17puDRAQYHdf+NPT\nYQ4KhvWBFK+a7G+sI0fDDNjPwlSfTMmzMH6EI2QivRh8a411YBwqIiIRmJ9XFZRPn0ZlkzAElJVB\nCg5G4Zp0fUc8Cq9NWpOSYY0dANOuf8K0cweu9OsPa79Y+X5T5gbhf2ci0nvrlUgYkIl0YvTpOtOO\nL+VgHJifB7RtiwaXiuWgbPr3v6oO1KMKmRonO6GhsN43Qv4xZPdOmHbvgjV2AKz9B1Zt9RL8d0Zi\nY1KXjnwuQUMhThOJ3CRyPyv9XrVmWr8WV/rEIvK2m+Xb8o6cRMhXu3Rtvzy9XKuqmC1Ie3Ny0OTJ\nx9Dwk9Xyz3knsyFVSmj86ku4/MbbSr2FOon8WfYleiV1ebSGfPnyZcyaNQtFRUUoKyvDY489hqio\nKMybNw8AEBMTg5deesmjxpJ/k79IN6yr+4sUMESgclWd78VA03XWpOSqEWcNYU+mVv3udKTW2qTp\n0zS7YAwAYVMmAgEBMO3cgfI77zbM747E49GU9caNG3HDDTdg9erVeOedd/DKK6/glVdewXPPPYc1\na9bg0qVL+Ne//qV0W8kPiL43l2pwVDrTg2lhpauWebI26bANFgtMGenyzbY1ZNuasrX/QH4+ySse\nBeTmzZujsLAQAGA2m9GsWTOcPXsWnTt3BgDExsZiz549yrWS/IdOpfSEL2HphB7tr70Gjqaer4Er\nXbXMk/5w1obGL8yBadc/5TVj2xqyjTV5uPCZ8SQ2jwLykCFD8Ntvv2HQoEFISUnBzJkzER4eLt8f\nERGB3NxcxRpJfkbjvbnCl7B0Qq/2K1lHWsmZEU/7w1kbLr/8WtX7/dunKP7wI7vHmt96h1ueyHuS\nBzIyMqS5c+dKkiRJP/74o9S/f38pOTlZvv+bb76RZsyY4fR5ysrKPXl58nWlpZKUmChJVVWSq/4l\nJlbdrvbrJSZKUmGh/c9qva5SjN5+G6V+7970hytt0PrzSX7DoyzrF198ET179kRcXBwAoFevXggM\nDJTXjTdu3Ijjx49j1qxZDp/H37MFmTFZBxVK6bnUz4KXsHRKgPYr8XkOMBchskYhD49LbjrpD0fZ\n7VcGx9ffBp1LPfI7QxuGKp3Ztm1bHDp0CABw9uxZNG7cGDfddBP27dsHANi2bRt69+7tyVOTn9Nt\nb67gJSydMnr7AWULeTjoD2dT2k2TE+ttg1H3jhs9R8JfeBSQR48ejbNnzyIlJQVPP/005s2bh+ee\new4LFizAmDFjcP3116Nnz55Kt5X8gFrXt3XK6Fec8YH2K1rIw0F/OFortl2lqr42qPH5VDtYGj1H\nwp+wMIiOOP2kDaf9LOgVZ1wmSPu9+TwrWsjDlf4ArprSLuvYSQ7GShcTUep9e9THgnw+jMRQU9ZE\nvsSdaUgRp/6MOo1ak5IjT5f6o44p7aLMLZrPzmiy716nrYTkPo6QdcQRsjZc6WdXSliqWY7RWyKU\n4BTp8+zpdZx1CVButMWbPlYsYc4P6DVCZkDWkUhfYL5MsX7m1J9Dhvk8C/h7dDVYetzHIp2AGACn\nrIlEx6k/nyDcFL/aCXkGv8ynP2FAJnKHL2wvqibCergebdA6k99ZfWy1g6VwJyBULwZkIncYfXtR\nNRG2wujZBk8uPOEJl+pjqxwsddtKSG5jQCZylQ9N/YlwVS0R2qA2l+tj16ggBsA+WCowY6DVCQh5\nh0ldOjJMEozBKdXPImdZe0ThRB9v98gq0QYhufgeXfl8hT/6ML8zNMAsaz/EgKwNJftZhO1FSlJy\nK4yn/ewr23E8ro9d41hn2d9RbaL4naEBZlkTGYC3U38iJFLVfF3d18NFaIMCvKmPLWMWv99jQCbS\niAiJVDIN18PrPQn5NM0v1uSd1ce240NZ/OQ+BmQijYiUxKTVVhiHJyHTU31nO049o9uyjp0QmJ/n\n+nv0kRkD8gzXkHXENWRtCNXPAiUxKb0eXmc/O1kXtd57H6zDRvr0mnzItq2u9TPXkIXBpC4/JFSg\n8GGi9bMaSUwiJJvV288CnYSoysv3ySxrcTCpi8gfqDAlqfbatNeJaP6wLqrAmjwLeBADMpFWVEqk\ncrY2jbIyjwOqIsHeD9ZFlVqTZwEP/8aATKQR1RKpHGyXsd57H8Knp3ocUL1ORPPwJETN7WFqPDdH\nt6QEriHrSLS1TV8lUj+rudZb59p0iMn7Sw26uDZaVz97Ut1MzYpoRq+2JtJn2ZcxqcsP8Y9LG37R\nz46CJuB1UpUriWj19bPbJyHmfm6ZAAAekUlEQVRqXq9YwGshu8MvPssCYFIXEXnG2bQw4F1SlZdr\nwG6vi6pZscro1bA+/licSm+kOAZkPyRU+UbymtO16Y3rPQ+oLqwBq/J5UjMz26BZ36b1a4GUFDEq\nvZEqGJD9jFDlG0kRDhOK3lkK02cZHmd2Owv2jV+Yg/DUKWiWMAAoLKx6UM3r/c58yrOArWZmtkGz\nvq1JyUBiohCV3vxFqbUc1rIKzV6Pa8g60mU9yOBraJ7w53U3JZKYHK4BJyWjWcIABB89DERFIW/3\nfxA2/TG5hnNgfh4AuPf6XEOuV1RYMKz3Jvt+kRUdXTRbsHb3Kez94QIAILxxCBY+3kux52dSl6B0\nCxT+Ujmpmj8HZECDKl6FhWjRo5scfAHIwdjafyAgSTDt+qfLAdCVkwgAHr0nX8iyzjt1xicuVymS\nCxdL8NEX/8Px7MKr7uvT5RpMTOig2GsxIAtKz0DhK9egdYW/B2QtBOReQGTHW+xu8ybL29FJBACv\ngqoIZUY9xRGycrJzLmHF5h/wa86lq+5rEAA8MrQj/u8P0QgICFD0dRmQBcURsjYYkFVWx+cJAPKO\nnIQUHQ1A4RNAg087e8xiQdTUicCWLf71vhV08mwRln12FLmFV+cLNDIF4ZGht6LzTZG6bXsKUuUV\nSVwOvszCJ6Xwj5rcUzMYR0UBubnyXc373o2Lew4AoaF1JlF5/FmrTlizva6pOtD7elAybc6Ug7Ht\nfdr1gwFG+Hr4x86T2Prdr3XeFxFuwsNJtyLm+uYat6puHCHrSI+Rm9HX0DzBEbJ6bJ8nec144GAU\nL1yK5n3vRmB+Hspu7YjKlq3cWkN2lT8tu9hEbduE3D6DDTndrqXZH+xBTkFpnfddG9UYDw+5FW1b\n1T9SZaUuP6R2oKhvrazxC3Nw+eXX/OaPmgFZXY1nPoVGq1YAiYnIfX9V1eeqsBDNhg2pyr6Gm1nW\nrvCzZRcbfpbrVilJePj1XQ6PmTX2dpdHwgzIfkjNX7o/joTrwy8x9ZnWr0X4QynILS77/cYaSViK\nJlH56xoy+Fmuqay8En/8626Hx8xJ6YZbrmvm9nMzIPshVf+4/PhLqzajfokZLRtYq37255NNo36W\nlXLZUobHF/7b4THPjumKDu1aePU6TOoiZflp4ouvkIPOhnV1Bx3A66BjtIBvYx05uur912x79edd\n9LaT+y6aLXhm6X8cHvPSpDvRJrqJRi1SDwOyL6uu2WuqkfhihJq9VH0d4g3r5Oz32jMc3pZJ1CLg\nq6nOtjm6YAUZypmcS3hh5XcOj/lrak+0CPet7zIGZF9WT81efx0huzoiFGLkqPIMh9oBn8hdP/5y\nEW+u+a/DYxY/2RuNQ4M1apH2GJB9Ffcb23F1RKjVyNGloK/mDAeXNEgAX3x7Gut2n3J4zAfP9EVw\nUKBGLdIXA7KPqn2VHn8vIuDqiFCLkaPLQV/tGQ4uaZAOVn3xP3x16DeHxyyfFYsGCpesNAJmWetI\nr33I/haM5X52de+q2ntcXcmAB9TPklf4ffp7BrAWjNrHr6Xtx4kzRQ6PWTm7v0atcY7bnvyQUf+4\njKZmP7ta3Un1KlBOgqHqW3tU2BbHz7P6jNTHj739FUqt5Q6PESkI18RtT0Rqc3UKWItkOCfTxWpv\n7fGVJQ3OAoll0l92Ory/eZgJbz12j0atMR4GZPIPria5aZUM50LQV3Nrj6h7ed0JsEbfuuULJEnC\nZCclK2+7sQVm3N9VoxYZWwO9G0CkhdojQim8Kcwr02AdOFgeEbpznFdqBf28k9ny84dPSgEsV18a\nTg3WkaOvPrnQcS+vLcDa9UF1X4WnToFp/Vq7461JyXb9FmAusutXbt1SR3lFJSb9ZScm/WVnvcG4\n800RWDm7P1bO7s9g7AauIevISOtBRmbrZ1H2Iftq6UevP8+erGv72UUm3OljJT/Hl0rL8MQ7jktW\nJt7dFiP73eTW84qKSV1+iAFZGyL2sy+ufSrSzx4EWH+6DKOrfazESd+5/Mv407Ish8c8lPAH9O7S\n2q33YAQMyH5IxEChFJECjij9LFKfqEGpfnYrwHKEXDcPs+j/ezIPi9Z/7/CpZ4zugttuiPD0LRgC\ns6zJZxgp2UarIGmkPtGVOxnurEZXPzcqsW346hQ2/+e0w6d7ceL/oW2r+gMJKYNJXaQ4b5JtTOvX\nXp3UZLFcldCjBHeTiLx5HevAOPs+yb2AZgkDmIBUk5vJbpok4BlZ9da6mmxb69789KCcmFVfMH7j\n0R5yYhaDsTY4QibluVkn2TZKNW3O/H0UuXQ5TDu+hDUpWbVRpKZlMgcOhnnpcoSnPlzVJx1vAQCU\ndezk3yO5GtzdGy3q1i1h1JptuHdGBrB0v8OHLJ3RB6EhDAt64RqyjkRZ21SLK2uB9QWsiohIBObn\noaxjJwQfPaxeSUeNy2ReevNtRNzeUb477+gJSFEtvX8dhXkyla/E59nX19m95e4a8sjOqU4PXT4z\nFg0a+F/daEeY1OWHfDoge1A32jpwMIrfWYLmfXogMD/P8ePc4Kyf9SiTaSNiApKnGbo+/XkWhCt9\n7KxaFgB8fHMuT3Ac0Csge7yGvGnTJgwdOhTDhw/H7t27ce7cOYwfPx5jx47F9OnTceXKFU+fmozO\nnbXA6ilG2/2RHW+xC8aAylcgqieJSNHiHKGhKF641O6m/INHdSkG4goW3DAe23qwo2C8cnZ/rHyy\nJ4OxwDwaIRcUFGDMmDFIT09HSUkJFi9ejPLycvTp0wcJCQlYsGABWrVqhbFjxzp8Hn8/m/bVEYUn\nI6zao9SaVBshq3CBhTpZLGiWMADBRw/LN9WeoheuGIgHU/m++nkWSc0+dmUkLOrFG0RnqBHynj17\n0KNHDzRp0gTR0dGYP38+srKyMGDAAABAbGws9uzZ41lryfCsI0fDvHSZ/Zd39Ui4zsBTxyi1IiIS\neUdOOh1FepOVLScR9R9on6Xbf6DiZTKDjx5GWcdO9u8p9WGYly4XLxgDDjN0a9MyM96fVUoS7n06\n07WRcPU/MhaP0unOnDkDi8WCqVOnwmw24/HHH0dpaSlCQkIAABEREcjNzVW0oWQsLl8YocZIzJbA\nZUvoCnsy1T4zuY4Sl4rs7a09SaRgWoXTzOEdX4oXjAGX9wPX/B3gs0z5sdxfrQxXSlYCHAn7DMkD\nH3zwgfTHP/5RKisrk06fPi317dtXuuuuu+T7f/nlF2n06NFOn6esrNyTlydfkpYmSYAkJSZKUmlp\n1c8FBVU/A1U/226vrbT09+MSEyWpsND+59JSx6/t7ePrez+1H7dyZdW/2q9d13sSgTv9okYf+rlj\npy9KSTMynP4j3+PRGnJ6ejry8vLwxz/+EQAwZMgQWCwWfP755wgNDcV3332HtLQ0LFq0yOHz+Pt6\nE9fcqni11cWFtU6ttj35ykUj3H4ffla+Ug3b9mZjzT9PODzmmohG+PC5QfzO0IChtj1duHABs2fP\nxooVK1BUVIThw4ejV69e6N69O5KTk/HnP/8ZMTExGDVqlMPn8fcPFgOyMpxtW9Ji25OtGpdtet22\nhavp/cO93ketB3dPkvzpAg9KeWvNQRz9pcDhMX27tsaD8X+Qf+Z3hjYMVcu6ZcuWiIuLw/333w8A\nmDt3Ljp16oRZs2Zh7dq1aN26Ne677z7PWkvkDndqH6vxePhmNS6XcwAARfrQX7iSGe2rV1Ai51gY\nREc82/WSi9uWVN/2JHg1LlWrX9Wcrk5MRN6iD9TZOmZgrgTh5x/sjhuuCXd6HL8ztGGobU9Eeqi9\nvcaWwWwbgbp7cQHFLk5Qq7hJzWAMAGHTH9Ot8IfaF9Co2YdIT+cFHqq5Uqhj0fTe8vYkV4Ix+T6O\nkHXEs13X1ZdoZCu4YZdoVGv056iflRw9BuTkIPK2m+Wf8w8eRZNnn9J3tKhB8RNbH0a1ifq9n/2w\n/rQrI+Hls2LRIMDzutH8ztCGoZK6lOLvHyz+cbnBi8CiST+LXI1Loyxof/w8a10tyx/7WA+GSuoi\n3yT0lXbcvKSjpmpV4ypauxFhT6baVePStQBIddUtU40saFXrg/s4lqwktTAgEwAFq16pSdDAInw1\nLmZBe41BmLTApC4CoP4VfhSpd6zFlZk84HbtbhcoVh/anStvkaysvML1KyixbjQphCNkqqLilLAi\no28Ha8jhk1J0H+25tW/XCSVnK5yO3kVYjhDE+YsleO7Db50ex+BLamFSl45ETNBQpeKSApm+3pSl\nFLGfHVI4M1qr3ADD9TOAPUfPY9lnPzg9TpQgbMQ+NiJmWfsh4f641MzGVeC53Q0sht6OY8D60MJ9\nnuuxOP17HDyR5/Q4UYJwTUbpY6NjQPZDQv1xabBfVct6xzVH1KbPMpFbXGa4Cz0YrT60UJ/nWlxJ\nymp/XVPMTrlDg9Z4TuQ+9iXc9uQhobfqGIjqa40aZ/pak5Jh3bCuaoQ5YgQCapV09DZJTXXMjPaa\nK0F4VL+bkHB3Ww1aQ+ScoQOyIbbqGIR15Oiq/qp5clMdlJUIxponZNU8odiyBZFbBNq37IzgCWwi\ncyUIz53QHTe2ZqlKEo+htz2pvVXH19XeWmMLunZbaxxkCru6NUexmtHuqt63XJMI+5Zt6uu/xi/M\n0ae/DMqV7UnvPtlH3p7EYEyiMvQIWejqTYLzdnbBncerOvp2ROBpX2f9VzJxMi6//Jq2/WUgroyE\nV8yKRYAXdaOJtOYTSV1GS36x0TVBw9skLg2SwLwi+mUBRe8/D6j9eWa1LCZ1aYVZ1p4y4PYQG93/\nuLztO4H73hBZ1gL3nyfU+DwzCNvT/TvDTzAge8LgowwR/ri8nV0QeXbCk33IWmfti9x/7lLi8yxJ\nEia/vsvpcf4UhGsS4TvDH3DbkwdYFtBL3q6xCrxGC7hfzlLzrH3B+08rxSVXMH3R106P89cgTP7D\n2FnWKhT19zX1ZkJ/mmY3u+D2RQd88KIFmmbt+2D/uePIz/lyZrSjYMyLN5A/MfaUtcG5My3iyVSq\ns/rPAOq9z9kJjTe1pbXm1vSTRuu6Ruo/Vznr51Vf/A9fHfrN6fMw+NaPU9ba4BqyH3L1l+7xl7eT\nNXbrvffBOmykx+ulRqmS5u4fl1brukbpP1fV1c+uJGUBDMKuYkDWBgOyH3L5l+5N8pqPZfJ6QsQR\nsi+y9bMrQbh5mAlvPXaPBq3yLQzI2mBA9kNaBQpfyuT1hCYnPn7OlSB8X+8bMPSeGzRoje9iQNYG\ns6zJseoykKYagdWlMpDM5HWZiFn7Ik9ruxKEX5jYHe1asVQlkSsYkI3Ck8DKixS4RbcSn/UQ8eIp\nrgTh92b0hSkkUIPWEPkWTlnrSO2pVF/M5PWEYaf5BJlCd7ValmH72UDYx9rglDXVy9OpVNFGfOQm\nHS+ewpKVRNrjCFlHau9DpipGH1VolZTnbRA2ej8bAftYGxwhk0PuloEkH6FiUl5lpYSH32DdaCJR\nMCATiUqFpLzcwlLMen+P0+MYhIm0x4BMJCiltmHtOXIeyzb/4PQ4BmEifTEgEwnKm6S8t/9xCId/\nynf6GgzCROJgQCYSmDu5A6wbTWRsDMg+htnY/sWVINy+TTPMHtdNg9YQkTcYkH2IiJWdSHmuBOGJ\nCX9Any6tNWgNESmFAdmHWJOSYd2wTs7Cvepyi0nJejfRa/XNADR+YQ4uv/xanTMDePRh7RuqMFeC\n8Gt/vBstmzfSoDVEpAYGZF+iY2UnLdQ3A9AsYQCCjx5G8N7vUPjFP6+aGUB4Q2DwUL2b7zZXgvCy\nmf0Q2KCBBq0hIrUxIPsaT68KZQD1zQAEHz2MiohIBB89XOfMgGnECKC4TO/mu4QlK4n8FwOyr/Hl\nyy06mgFYuhzhqQ/XOTMQFRoqdEBmECYigLWsdaV4vVRBrg6ktvpqO9d3u4j1f30xCIvYz76GfawN\n1rImrylV2Ulo9c0ALF2OsNSHr759ZRqA+v8AtFJeUYlH3tzt9DijBWEiUg4Dsg/x+cstOpgBaNGj\nGwLz8+qs+YzPMnVpbk5BCWZ/8K3T4xiEiQjglLWuOP3kHjnLuub0e40s67KOnerOsk5LQ65GWdb7\n/peDpRlHnB7ni0GYn2f1sY+1wSlrIifqmwEo/OKfV+9DrjEzED5uHKDil1jatmPYeeCs0+N8MQgT\nkXIYkMlQ6qvtfPmNt+u8Xa1p+tQF/4LlSoXDY1q1aIRXH7lbldcnIt/DgEwe8cea2a5kRo/ufzPi\n7rxeg9YQka9hQCa3+VPNbFeC8NwJ3XFj63ANWkNEvowBmdzm6zWzXQnCi5/sjcahwRq0hoj8BQMy\nuc8Ha2a7EoRXzIpFQECABq0hIn/kVUC2WCxISkpCamoqevTogZkzZ6KiogJRUVF48803ERISolQ7\nSTQ+UDPbF6tlEZFxeRWQ33vvPTRt2hQAsGjRIowdOxYJCQlYsGAB1q9fj7FjxyrSSBKQQWtmMwgT\nkag8DsinTp3CyZMn0a9fPwBAVlYWXnrpJQBAbGwsVq5cyYDsqxxUzAqflCJcUL73aeeVuhiEiUhv\nHgfk119/Hc8//zwyMjIAAKWlpfIUdUREBHJzc5VpIQlH9JrZV8oqMPWtfzk9jkGYiETiUUDOyMhA\n165d0aZNmzrvd7UaZ/PmjRAUFOhJE3yGozJqwnr0YSC8IUwjRlRd2hAAEFZVMzo9vaoylsbO51/G\nlFd3OD3us7eMnQEuOkN+ng2GfawNPfrZo4C8e/duZGdnY/fu3Th//jxCQkLQqFEjWCwWhIaG4sKF\nC4iOjnb6PAUFJZ68vM8wdF3awUOrrjFc+zrDg4eqWqaypiM/5WPBPw45Pe6zt5LlfjZsfxuAoT/P\nBsE+1oahalkvXLhQ/v/ixYtx7bXX4uDBg/jyyy+RnJyMbdu2oXfv3p48NZFDOw+cQdq24w6Pub5l\nE8x76E6NWkREpAzF9iE//vjjmDVrFtauXYvWrVvjvvvuU+qpyc99sOkosn644PCY4X1uRFLPdto0\niIhIBV4H5Mcff1z+/0cffeTt0xEBAOZ8sAcXCkodHjNr7O2Iub65Ri0iIlIXK3WRMFzZI7xg2j1o\n1sSkQWuIiLTFgEy6ciUIf/hsPwQFNtCgNURE+mFAJs2xWhYR0dUYkEkTDMJERI4xIJNqGISJiFzH\ngEyKkSQJk1/f5fQ4BmEioqsxIJNXrGUVeJR1o4mIvMaATG4rKLbi6SXfODymoSkIS57qo1GLiIiM\njwGZXPLzOTPm/22fw2O6tY/CtOGdNGoREZFvYUCmen179Dw+/OwHh8eMH9wesd2u06hFRES+iwGZ\n7Hzx7Wms233K4THPPnA7OrRlyUoiIiUxIBPezzyC737McXjMX6b2QHSzhhq1iIjI/zAg+6kVm3/A\nN0fOOzxmyVN90NDEjwgRkRb4betHXlz5HbJzLjk8ZvmsWDQICNCoRUREZMOA7ONWfP4DvjnseCTM\nPcJERPpjQPYhpvVrYU1KRua+c8j8+meHxzIIExGJhQHZR2xftgmf5kcBC/9T5/3XhZTj5RmDNW4V\nERG5igHZoCRJwrrdp7A169fqW5pcdcyUM//C0H+8DevAwTCvTNO2gURE5BYGZAORJAk/ni7AX9f8\nt95j3jj+D3TY/In8sxyMQ0O1aCIREXmIAVlwlZKEk2eKsP9YLg4cz0G+2XrVMa89cjdatmgEAAgw\n3wHUCMjF769gMCYiMgAGZAFVVFbi2K+F1UE4F0WXrwCoumDDnR2iERIciGG9b0TzMJP9Ay0WhE2d\nbHdT2NTJHCETERkAA7Igysor8ePpi9h3LBf/PZGHS6VlAIAmDYPRu/M1uCMmGre2a46gwAZ1P4HF\ngvBJKTDt2AbrwMEofn8FwqZOhmnHNoRPSmFQJiISHAOyjixXyrH/WC72H8/BoZN5KLVWAACaNglB\n/27X4o6YaLRv0xSBDeoJwjWYNmfKwdgWfM0r0+QgbdqcCevI0Wq/JSIi8hADssZKreX4/lQ+9h/L\nweGfL8J6pSoIR4SHonfn1ugeE40brw13u1qWdeRomAFYk5J/HwlXB2UGYyIi8TEga+BSaRn+eyIP\n+4/l4OgvBSivqAQAXBvVGF1vjsQdMVFo2zIMAV6WrKwz6IaGMhgTERkAA7JKii5fwcHjudh/LAf/\n+7UQFZUSAOC6qMa4IyYad8REoWuHVsjLc1xbmoiI/AMDsoIumi3YfzwX+4/l4sSZQkhVMRjtWoXh\njpgo3BETjVbV25MAeD0iJiIi38GA7KWcwlLsP5aD/cdy8dNvZgBAAICbrmuK7u2j0C0mCpFNeR1h\nIiJyjAHZA7/lXZaD8K/VlzNsEBCADm2b446YKHRrH4VmTUxOnoWIiOh3DMgukCQJ2TmXsO9Y1Zrw\nufwSAEBggwB0ujECd8RE4fZbIhHWKETnlhIRkVExINdDkiT8dM5ctU/4WA5yCy0AgOCgBrj9lkh0\nj4lGl5sj0Cg0WOeWEhGRL2BArqGyUsKJM4XVxTpyUVBcVTfaFBKIOztE446YaHS6sQVCQ9htRESk\nLL+PLOUVtrrROThwPBfmkqqSlY1MQeh5WyvcEROF225ogeCgQJ1bSkREvswvA3JZeQWO/lKA/cdy\n8N8TebhsKQcAhDUKRp8urdE9Jgp/aOugbjQREZHC/CYgW69U4PBP+dh3LAffn8qHpbpkZfMwE+7u\n2ArdY6Jwy3XN0KAB9wYTEZH2fDogl1jKcehUHvYfy8WRn/JxpbyqZGVk01D063ot7oiJwg2t3a8b\nTUREpDSfC8iXSsuqSlYez8UPv1xEeUVVuaxrIhpVVctqH43rWzZhlSwiIhKKzwTk/CILVn3xI348\nXYjK6pqVbaKbyCUrr41srHMLiYiI6uczAfncxcs4+ksBbrgmHN1jqkpWtmzeyPkDiYiIBOAzAfm2\nGyKwbGY/BDZgZjQRERmPT0UvBmMiIjIqRjAiIiIBMCATEREJgAGZiIhIAAzIREREAmBAJiIiEgAD\nMhERkQAYkImIiATAgExERCQABmQiIiIBMCATEREJgAGZiIhIAAGSVH2tQiIiItINR8hEREQCYEAm\nIiISAAMyERGRABiQiYiIBMCATEREJAAGZCIiIgEE6d0Af1FaWorZs2cjPz8fVqsVqamp+MMf/oCZ\nM2eioqICUVFRePPNNxESEqJ3Uw3PYrEgKSkJqamp6NGjB/tYYVlZWZg+fTpuueUWAED79u3x8MMP\ns59VsGnTJixfvhxBQUF44oknEBMTw35W2Lp167Bp0yb55yNHjuDTTz/FvHnzAAAxMTF46aWXNGkL\n9yFrZMuWLTh79iymTJmCs2fPYtKkSejWrRv69OmDhIQELFiwAK1atcLYsWP1bqrhvf322/j6668x\nbtw47N27l32ssKysLHz88cdYtGiRfNucOXPYzworKCjAmDFjkJ6ejpKSEixevBjl5eXsZxV99913\n+OKLL3Dy5Ek8++yz6Ny5M55++mkMHToUffv2Vf31OWWtkcTEREyZMgUAcO7cObRs2RJZWVkYMGAA\nACA2NhZ79uzRs4k+4dSpUzh58iT69esHAOxjjbCflbdnzx706NEDTZo0QXR0NObPn89+VtmSJUvk\nQVPnzp0BaNvPDMgaGzNmDJ555hk899xzKC0tlaebIiIikJubq3PrjO/111/H7Nmz5Z/Zx+o4efIk\npk6digceeADffPMN+1kFZ86cgcViwdSpUzF27Fjs2bOH/ayi77//Htdccw0CAwMRHh4u365lP3MN\nWWNr1qzBjz/+iGeffRY1Vwu4cuC9jIwMdO3aFW3atKnzfvaxMtq1a4dp06YhISEB2dnZmDBhAioq\nKuT72c/KKSwsxLvvvovffvsNEyZM4HeGitavX49hw4ZddbuW/cyArJEjR44gIiIC11xzDTp06ICK\nigo0btwYFosFoaGhuHDhAqKjo/VupqHt3r0b2dnZ2L17N86fP4+QkBA0atSIfaywli1bIjExEQBw\n/fXXIzIyEocPH2Y/KywiIgK33347goKCcP3116Nx48YIDAxkP6skKysLc+fORUBAAAoLC+Xbtexn\nTllrZN++fVi5ciUAIC8vDyUlJejZsye+/PJLAMC2bdvQu3dvPZtoeAsXLkR6ejr+8Y9/YNSoUUhN\nTWUfq2DTpk1YsWIFACA3Nxf5+fkYPnw4+1lhvXr1wrfffovKykoUFBTwO0NFFy5cQOPGjRESEoLg\n4GDceOON2LdvHwBt+5lZ1hqxWCz405/+hHPnzsFisWDatGm47bbbMGvWLFitVrRu3RqvvfYagoOD\n9W6qT1i8eDGuvfZa9OrVi32ssEuXLuGZZ56B2WxGWVkZpk2bhg4dOrCfVbBmzRqsX78eAPDoo4+i\nU6dO7GcVHDlyBAsXLsTy5csBVOVIvPDCC6isrESXLl0wZ84cTdrBgExERCQATlkTEREJgAGZiIhI\nAAzIREREAmBAJiIiEgADMhERkQAYkImIiATAgExERCQABmQiIiIB/D/3Yu0dTC4TqAAAAABJRU5E\nrkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f28d5dd8828>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "3426830.249861087\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mORdoeq7U-Ps",
        "colab_type": "code",
        "outputId": "777c0d51-a481-432c-c305-318d30f07df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# Softmax Classifier\n",
        "import numpy as np\n",
        " \n",
        "def softmax(x):\n",
        "    e = np.exp(x - np.max(x)) \n",
        "    if e.ndim == 1:\n",
        "        return e / np.sum(e, axis=0)\n",
        "    else:  \n",
        "        return e / np.array([np.sum(e, axis=1)]).T  # ndim = 2\n",
        "\n",
        "class LogisticRegression(object):\n",
        "    def __init__(self, input, label, n_in, n_out):\n",
        "        self.x = input\n",
        "        self.y = label\n",
        "        self.W = np.zeros((n_in, n_out))  # initialize W 0\n",
        "        self.b = np.zeros(n_out)          # initialize bias 0\n",
        "\n",
        "    def train(self, lr=0.1, input=None, L2_reg=0.00):\n",
        "        # P_y_given_x는 x데이터를 넣어 추측한 Y의 확률(Probability)이다\n",
        "        P_y_given_x = softmax(np.matmul(self.x, self.W) + self.b)\n",
        "        diff_y = self.y - P_y_given_x\n",
        "        \n",
        "        self.W += lr * np.matmul(self.x.T, diff_y) - lr * L2_reg * self.W\n",
        "        self.b += lr * np.mean(diff_y, axis=0)\n",
        "        \n",
        "    def negative_log_likelihood(self):\n",
        "        sigmoid_activation = softmax(np.matmul(self.x, self.W) + self.b)\n",
        "        cross_entropy = - np.mean(np.sum(self.y * np.log(sigmoid_activation) + (1 - self.y) * np.log(1 - sigmoid_activation), axis=1))\n",
        "\n",
        "        return cross_entropy\n",
        "\n",
        "    def predict(self, x):\n",
        "        return softmax(np.matmul(x, self.W) + self.b)\n",
        "\n",
        "\n",
        "# training data\n",
        "x = np.array([[1, 0, 0],\n",
        "              [0, 1, 0],\n",
        "              [0, 0, 1]])\n",
        "y = np.array([[1, 0, 0],\n",
        "              [0, 1, 0],\n",
        "              [0, 0, 1]])\n",
        "\n",
        "# construct LogisticRegression\n",
        "classifier = LogisticRegression(input=x, label=y, n_in=3, n_out=3)\n",
        "\n",
        "# train\n",
        "# Hyper_parameter도 한번 바꿔보세요\n",
        "n_epochs = 1\n",
        "learning_rate = 0.2\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  classifier.train(lr=learning_rate)\n",
        "  cost = classifier.negative_log_likelihood()\n",
        "  print('Training epoch %d, cost is ' % epoch, cost)\n",
        "  learning_rate *= 0.95\n",
        "\n",
        "# test\n",
        "# test에서 one-hot encoding을 추가해보세요!\n",
        "x = np.array([1, 0, 0])\n",
        "print('predict : ', classifier.predict(x))\n",
        "x = np.array([0, 1, 0])\n",
        "print('predict :', classifier.predict(x))\n",
        "x = np.array([0, 0, 1])\n",
        "print('predict :', classifier.predict(x))\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch 0, cost is  1.7131729728567044\n",
            "predict :  [0.37915245 0.31042377 0.31042377]\n",
            "predict : [0.31042377 0.37915245 0.31042377]\n",
            "predict : [0.31042377 0.31042377 0.37915245]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YkEyqOtkVDDH",
        "colab_type": "code",
        "outputId": "90ca9cd3-06e0-45e3-8fa0-46ce0dc21b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        }
      },
      "cell_type": "code",
      "source": [
        "# Neural Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters(사용자가 지정해주는 파라미터)\n",
        "#input_size는 28*28 이미지 -> 784차원의 벡터로 표현 가능\n",
        "input_size = 784\n",
        "# hidden_size는 중간 hidden layer의 depth\n",
        "hidden_size = 500\n",
        "# 최종 class(10개)\n",
        "num_classes = 10\n",
        "# 몇번이나 데이터셋을 돌릴것인가? \n",
        "num_epochs = 5\n",
        "# 한번에 학습시키는데 몇장의 이미지를 넣을것인가?\n",
        "batch_size = 100\n",
        "# 코스트 함수에서의 알파값을 의미\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "# Fully connected neural network with one hidden layer\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # neural net 설계\n",
        "        # nn.Linear는 fully cunnected layer를 의미한다\n",
        "        # nn.Linear의 인풋 파라미터로는 입력depth와 출력 depth\n",
        "        # 만을 입력해주면 알아서 해준다\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
        "        # relu 함수를 통해 다음 레이어로 feature를 전달해준다.\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    # 앞으로 전달\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "# Cross-Entropy을 loss로 사용\n",
        "# optimizer는 Adam이라는 optimizer로 성능이 좋아 기본으로 사용된다.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Train the model\n",
        "# len 함수를 통해 데이터셋의 총 갯수를 알수 있다\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    # enumerate 함수는 train_loader에서 i와 같은 카운트를 할 수 있도록 도와줌\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # optimizer의 gradient를 초기화해준다\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagation을 해준다\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "# 정확한 accuracy 측정을 위해 test셋을 이용해 test를 진행시킨다\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.3152\n",
            "Epoch [1/5], Step [200/600], Loss: 0.1460\n",
            "Epoch [1/5], Step [300/600], Loss: 0.2947\n",
            "Epoch [1/5], Step [400/600], Loss: 0.1806\n",
            "Epoch [1/5], Step [500/600], Loss: 0.1333\n",
            "Epoch [1/5], Step [600/600], Loss: 0.2735\n",
            "Epoch [2/5], Step [100/600], Loss: 0.2332\n",
            "Epoch [2/5], Step [200/600], Loss: 0.1389\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0905\n",
            "Epoch [2/5], Step [400/600], Loss: 0.1161\n",
            "Epoch [2/5], Step [500/600], Loss: 0.1753\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0467\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0737\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0900\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0348\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0464\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0709\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0492\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0489\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0574\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0329\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0758\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0162\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0458\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0288\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0254\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0178\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0552\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0196\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0594\n",
            "Accuracy of the network on the 10000 test images: 97.96 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ot6YuoMFVDNc",
        "colab_type": "code",
        "outputId": "e7cd46ab-593d-4683-c2fc-b29ccf251a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        }
      },
      "cell_type": "code",
      "source": [
        "# CNN \n",
        "# 스스로 코드를 분석해보세요!\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper parameters\n",
        "num_epochs = 5\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "# Convolutional neural network (two convolutional layers)\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # nn.Sequential 모듈을 이용해 간단하게 표현해 줄 수 있다\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Linear(7*7*32, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "model = ConvNet(num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.2512\n",
            "Epoch [1/5], Step [200/600], Loss: 0.1171\n",
            "Epoch [1/5], Step [300/600], Loss: 0.0788\n",
            "Epoch [1/5], Step [400/600], Loss: 0.0769\n",
            "Epoch [1/5], Step [500/600], Loss: 0.0181\n",
            "Epoch [1/5], Step [600/600], Loss: 0.0281\n",
            "Epoch [2/5], Step [100/600], Loss: 0.0779\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0299\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0539\n",
            "Epoch [2/5], Step [400/600], Loss: 0.0143\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0116\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0337\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0334\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0297\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0156\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0257\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0255\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0133\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0249\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0029\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0267\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0207\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0674\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0192\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0097\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0022\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0173\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0506\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0401\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0032\n",
            "Test Accuracy of the model on the 10000 test images: 98.97 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ybzgg8GJIAYj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GAN 코드 스스로 분석해보기\n",
        "# 모르는게 있으면 구글링을 해보세요!\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "latent_size = 64\n",
        "hidden_size = 256\n",
        "image_size = 784\n",
        "num_epochs = 200\n",
        "batch_size = 100\n",
        "sample_dir = 'samples'\n",
        "\n",
        "# Create a directory if not exists\n",
        "if not os.path.exists(sample_dir):\n",
        "    os.makedirs(sample_dir)\n",
        "\n",
        "# Image processing\n",
        "transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels\n",
        "                                     std=(0.5, 0.5, 0.5))])\n",
        "\n",
        "# MNIST dataset\n",
        "mnist = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                   train=True,\n",
        "                                   transform=transform,\n",
        "                                   download=True)\n",
        "\n",
        "# Data loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True)\n",
        "\n",
        "# Discriminator\n",
        "D = nn.Sequential(\n",
        "    nn.Linear(image_size, hidden_size),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Linear(hidden_size, hidden_size),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Linear(hidden_size, 1),\n",
        "    nn.Sigmoid())\n",
        "\n",
        "# Generator \n",
        "G = nn.Sequential(\n",
        "    nn.Linear(latent_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, image_size),\n",
        "    nn.Tanh())\n",
        "\n",
        "# Device setting\n",
        "D = D.to(device)\n",
        "G = G.to(device)\n",
        "\n",
        "# Binary cross entropy loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
        "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)\n",
        "\n",
        "def denorm(x):\n",
        "    out = (x + 1) / 2\n",
        "    return out.clamp(0, 1)\n",
        "\n",
        "def reset_grad():\n",
        "    d_optimizer.zero_grad()\n",
        "    g_optimizer.zero_grad()\n",
        "\n",
        "# Start training\n",
        "total_step = len(data_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, _) in enumerate(data_loader):\n",
        "        images = images.reshape(batch_size, -1).to(device)\n",
        "        \n",
        "        # Create the labels which are later used as input for the BCE loss\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                      Train the discriminator                       #\n",
        "        # ================================================================== #\n",
        "\n",
        "        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
        "        # Second term of the loss is always zero since real_labels == 1\n",
        "        outputs = D(images)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "        real_score = outputs\n",
        "        \n",
        "        # Compute BCELoss using fake images\n",
        "        # First term of the loss is always zero since fake_labels == 0\n",
        "        z = torch.randn(batch_size, latent_size).to(device)\n",
        "        fake_images = G(z)\n",
        "        outputs = D(fake_images)\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "        fake_score = outputs\n",
        "        \n",
        "        # Backprop and optimize\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        reset_grad()\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "        \n",
        "        # ================================================================== #\n",
        "        #                        Train the generator                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "        # Compute loss with fake images\n",
        "        z = torch.randn(batch_size, latent_size).to(device)\n",
        "        fake_images = G(z)\n",
        "        outputs = D(fake_images)\n",
        "        \n",
        "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
        "        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "        \n",
        "        # Backprop and optimize\n",
        "        reset_grad()\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "        \n",
        "        if (i+1) % 200 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
        "                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n",
        "                          real_score.mean().item(), fake_score.mean().item()))\n",
        "    \n",
        "    # Save real images\n",
        "    if (epoch+1) == 1:\n",
        "        images = images.reshape(images.size(0), 1, 28, 28)\n",
        "        save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))\n",
        "    \n",
        "    # Save sampled images\n",
        "    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
        "    save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}